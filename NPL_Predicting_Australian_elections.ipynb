{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, para comenzar el proyecto instalamos todas las librerias y paquetes necesarios, en donde las más utilizadas serán numpy que nos permite realziar operaciones matemáticas de arreglos y matrices, pandas que permite gestionar dataframes y sklearn que permite gestionar preprocesamiento y algoritmos de machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez instaladas las librerías se procede a descargar la información csv, descargada en mi computador, únicamente se descargo la información de los tweets y no la correspondiente a la latitud y longitud de donde fueron realizados los mismos, ya que como se mencionará más adelante, esta no se considera necesaria. Una vez descargados los datos se verifica que hayan quedado guardados correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anmam'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datos=pd.read_csv(r\"C:\\Users\\anmam\\OneDrive\\Documentos\\Ejercicio Twitter\\auspol2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>user_screen_name</th>\n",
       "      <th>user_description</th>\n",
       "      <th>user_location</th>\n",
       "      <th>user_created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-20 09:13:44</td>\n",
       "      <td>1130401208756187136</td>\n",
       "      <td>After the climate election: shellshocked green...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.248486e+07</td>\n",
       "      <td>PIPELINEPETE</td>\n",
       "      <td>jocksjig</td>\n",
       "      <td>Retired Tradesman and Progressive Anti Conserv...</td>\n",
       "      <td>Brisbane, Queensland</td>\n",
       "      <td>2009-11-25 09:19:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-20 09:13:43</td>\n",
       "      <td>1130401205367140357</td>\n",
       "      <td>@narendramodi @smritiirani Coverage of indian ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.756474e+08</td>\n",
       "      <td>Narinder Parmar</td>\n",
       "      <td>nparmar1957</td>\n",
       "      <td>Life coach &amp; trainer, Motivational speaker, Ma...</td>\n",
       "      <td>Wollongong, NSW, AUSTRALIA</td>\n",
       "      <td>2012-08-23 10:20:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-20 09:13:33</td>\n",
       "      <td>1130401162782371841</td>\n",
       "      <td>@workmanalice Do you know if Facebook is relea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.687300e+04</td>\n",
       "      <td>Peter Wells</td>\n",
       "      <td>peterwells</td>\n",
       "      <td>Writes for @theage and @smh on technology and ...</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>2006-12-11 07:38:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-20 09:13:29</td>\n",
       "      <td>1130401143551434753</td>\n",
       "      <td>@vanbadham We all understand we have a compuls...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.081660e+17</td>\n",
       "      <td>The Realist</td>\n",
       "      <td>therealist822</td>\n",
       "      <td>Calls it as I see it. Anti PC, SJW and VS. If ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-14 03:10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-20 09:13:23</td>\n",
       "      <td>1130401118666809345</td>\n",
       "      <td>Shares were mixed in Asia, with India and Aust...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.260074e+08</td>\n",
       "      <td>Inquirer Business</td>\n",
       "      <td>InquirerBiz</td>\n",
       "      <td>The official Twitter account of the Inquirer G...</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>2012-03-16 03:51:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at                   id  \\\n",
       "0  2019-05-20 09:13:44  1130401208756187136   \n",
       "1  2019-05-20 09:13:43  1130401205367140357   \n",
       "2  2019-05-20 09:13:33  1130401162782371841   \n",
       "3  2019-05-20 09:13:29  1130401143551434753   \n",
       "4  2019-05-20 09:13:23  1130401118666809345   \n",
       "\n",
       "                                           full_text  retweet_count  \\\n",
       "0  After the climate election: shellshocked green...            0.0   \n",
       "1  @narendramodi @smritiirani Coverage of indian ...            0.0   \n",
       "2  @workmanalice Do you know if Facebook is relea...            0.0   \n",
       "3  @vanbadham We all understand we have a compuls...            0.0   \n",
       "4  Shares were mixed in Asia, with India and Aust...            0.0   \n",
       "\n",
       "   favorite_count       user_id          user_name user_screen_name  \\\n",
       "0             0.0  9.248486e+07       PIPELINEPETE         jocksjig   \n",
       "1             0.0  7.756474e+08    Narinder Parmar      nparmar1957   \n",
       "2             0.0  5.687300e+04        Peter Wells       peterwells   \n",
       "3             0.0  9.081660e+17        The Realist    therealist822   \n",
       "4             0.0  5.260074e+08  Inquirer Business      InquirerBiz   \n",
       "\n",
       "                                    user_description  \\\n",
       "0  Retired Tradesman and Progressive Anti Conserv...   \n",
       "1  Life coach & trainer, Motivational speaker, Ma...   \n",
       "2  Writes for @theage and @smh on technology and ...   \n",
       "3  Calls it as I see it. Anti PC, SJW and VS. If ...   \n",
       "4  The official Twitter account of the Inquirer G...   \n",
       "\n",
       "                user_location      user_created_at  \n",
       "0        Brisbane, Queensland  2009-11-25 09:19:45  \n",
       "1  Wollongong, NSW, AUSTRALIA  2012-08-23 10:20:40  \n",
       "2                   Melbourne  2006-12-11 07:38:06  \n",
       "3                         NaN  2017-09-14 03:10:30  \n",
       "4                 Philippines  2012-03-16 03:51:59  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a descargar la librería nltk, muy util al momento de gestionar textos y realizar preprocesamiento de datos que vienen en formato texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anmam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se guardan los textos descargados en el archivo en la variable textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After the climate election: shellshocked green...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@narendramodi @smritiirani Coverage of indian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@workmanalice Do you know if Facebook is relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@vanbadham We all understand we have a compuls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shares were mixed in Asia, with India and Aust...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           full_text\n",
       "0  After the climate election: shellshocked green...\n",
       "1  @narendramodi @smritiirani Coverage of indian ...\n",
       "2  @workmanalice Do you know if Facebook is relea...\n",
       "3  @vanbadham We all understand we have a compuls...\n",
       "4  Shares were mixed in Asia, with India and Aust..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textos=datos[['full_text']]\n",
    "textos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que se realiza es la tokenización de la variable texto ignorando los signos de puntuación y caracteres especiales tales como: ., ,, *, @, ;, etc; Para lo mismo se usa la función RegexpTokenizer.tokenize de la libreria NLKT, que permite ayudarnos a ignorar los signos de puntiuación y dividir los textos en cuestión por palabras. El resultado de este prcedimiento es test_tokens correspondiente a un vector de listas, en donde cada lista corresponde a las palabras que componen el tweet.\n",
    "\n",
    "Se realiza una impresión de los primeros datos de text_tokens para comprobar que el algorito está funcionanco correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "text_tokens=textos.apply(lambda fila:tokenizer.tokenize(fila['full_text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [After, the, climate, election, shellshocked, ...\n",
       "1    [narendramodi, smritiirani, Coverage, of, indi...\n",
       "2    [workmanalice, Do, you, know, if, Facebook, is...\n",
       "3    [vanbadham, We, all, understand, we, have, a, ...\n",
       "4    [Shares, were, mixed, in, Asia, with, India, a...\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se descarga de la librería nlkt las \"stopwords\" correspondientes a palabaras muy comunes y las cuales no aportan información significativa de la variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anmam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la variable palabras se seleccionan las \"stopwords\" del idioma ingles, que corresponde al idioma en que está escrito el texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea una variable llamada text_tokens2 en donde se colocan en minúscula todas las palabras de las listas guardadas anteriormente en text_tokens, esto con el fin de sean posibles remover todas las stopwords de las listas.\n",
    "Una vez todas las listas presentan palabras en minúsculas, se crea la variable tokens_despues la cual elimina todas las \"stopwords\" del idioma inglés de las listas. Se realiza la impresión de tokens_despues para comprobar que los algrítmos hayan funcionado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens2=text_tokens.apply(lambda func2: [i.lower() for i in func2])\n",
    "tokens_despues = text_tokens2.apply(lambda x: [i for i in x if not i in palabras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [climate, election, shellshocked, green, group...\n",
       "1    [narendramodi, smritiirani, coverage, indian, ...\n",
       "2    [workmanalice, know, facebook, releasing, elec...\n",
       "3    [vanbadham, understand, compulsory, preference...\n",
       "4    [shares, mixed, asia, india, australia, leadin...\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_despues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente se realiza una técnica que en el lenguaje común se conoce como stemming, la técnica consiste en reemplazar cada palabra por su raíz, por ejemplo, las palabras fishing, fisher, y fish quedarían todas bajo la palabra fish.\n",
    "\n",
    "Las listas después de realizar stemming se guardan en la variable result1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p1 = PorterStemmer()\n",
    "result1=tokens_despues.apply(lambda x: [p1.stem(i) for i in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [climat, elect, shellshock, green, group, rema...\n",
       "1    [narendramodi, smritiirani, coverag, indian, e...\n",
       "2    [workmanalic, know, facebook, releas, elect, p...\n",
       "3    [vanbadham, understand, compulsori, prefer, sy...\n",
       "4    [share, mix, asia, india, australia, lead, gai...\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el stemming a las variables resultantes se aplica la lemmatization, la lemmatization corresponde a reemplazar cada palabra con la palabra de origen, por ejemplo la palabra better se reemplazaría por good. Para realizar la lemmatization se utiliza la función WordnetLemmatizer.lemmatize de la librería NLTK.\n",
    "\n",
    "Los resultados se encuentran en la variable result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anmam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "result2=result1.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [climat, elect, shellshock, green, group, rema...\n",
       "1    [narendramodi, smritiirani, coverag, indian, e...\n",
       "2    [workmanalic, know, facebook, releas, elect, p...\n",
       "3    [vanbadham, understand, compulsori, prefer, sy...\n",
       "4    [share, mix, asia, india, australia, lead, gai...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez preprocesada la información del texto se crea procede a procesar la información de las etiquetas. Para este ejercicio en particular la variable de salidas será 0 si no hubo retweets y 1 si se presentaron retweets. Para lo mismo se utiliza la función numpy.gt, en la cual se marca como 0 cuando no hubo ningún retweet y 1 cuando se presentaron retweets. Los resultados se guardan en la variable respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuestas= datos[['retweet_count']].gt(0).astype(np.short)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez procesadas las respuestas se procede a verificar que otras variables serían útiles para el caso de estudio.\n",
    "Para lo mismo se consideran las variables que se tienen nombre de usuario, fecha de creación, fecha de creación usuario, localización, nombre en pantalla.\n",
    "\n",
    "Se considera intuitivamente que no hay razón  para pensar que por el nombre de usuario o localización se realicen más retweet, si bien es cierto que ciertos nombres de personas famosas tienen mayor probabilidad de ser retweeteadas, dado el alcance del ejercicio y que los tweets únicamente tienen en cuenta 10 días de historia, no se tendrá en cuenta este parámetro en las variables explicativas.\n",
    "\n",
    "La localización tampoco se tendrá en cuenta como variable, ya que no existe una razón intuitiva para pensar que en ciertos lugares que se realice el tweet haya mayor probabilidad de ser retweeteado, el lugar de origen del tweet no debería ser un factor determinante y solo agregaría ruido al modelo.\n",
    "\n",
    "La hora y el día en que se realice el tweet si se considera un factor determinante, ya que las personas estan más expuestas a twiter a ciertas horas del días y ciertos días a la semana, por lo tanto la probabilidad de retweet, puede estar determinada por la visibilidad del tweet. \n",
    "\n",
    "Dado que únicamente se cuentan con 10 días de historia únicamente se considerará la hora del día como un factor determinante y no el día de la semana, por simplicidad se ignorarán los segundos en el cálculo de la hora.\n",
    "La hora del día se calcularpa como Hora militar*60 + minutos. Para lo mismo se creará una función la sual dado un String extraiga la hora en el formato especificado.\n",
    "\n",
    "Es de resaltar que debido a que hay datos que carecen de hora en la base, se incluirá en la función que cuando no sea posible extaer la hora se generé un np.nan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def funcionfecha(y):\n",
    "    try:\n",
    "        x=y.to_string()\n",
    "        datetime_object = datetime.strptime(x, 'created_at    %Y-%m-%d %H:%M:%S')\n",
    "        return(datetime_object.hour*60+datetime_object.minute)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica la función creada (funcionfecha) al campo \"created_at\" de los datos, para extraer las horas de cada una de las observaciones, los resultados se almacenan en la variable horasa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "horasa=datos[['created_at']].apply(funcionfecha,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a dividir los datos obtenidos en entrenamiento y prueba, dado el volumen suficiente de la información y las recomendaciones de la literatura, se seleccionara 20% como datos de prueba y 80% como datos de entrenamiento.\n",
    "\n",
    "Es así que se crea un dataframe con los datos procesados de texto, hora y existencia de retweets, el arreglo resultante se llamará datosfinal.\n",
    "\n",
    "Usando la funcion model_selection de sklearn se realiza la partición de los datos en X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146703, 2) (146703,)\n",
      "(36676, 2) (36676,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35111</th>\n",
       "      <td>[bright, side, cancer, chariti, expect, influx...</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179673</th>\n",
       "      <td>[abctv, abccompass, watch, kumi, taguchi, late...</td>\n",
       "      <td>568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103060</th>\n",
       "      <td>[auspol, http, co, btpyxpfx7n]</td>\n",
       "      <td>1376.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62430</th>\n",
       "      <td>[look, like, hung, parliament, mojonewsau, aus...</td>\n",
       "      <td>668.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176772</th>\n",
       "      <td>[worst, part, elect, tri, figur, put, last, ua...</td>\n",
       "      <td>799.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39349</th>\n",
       "      <td>[smh, jacquelinemaley, politician, auspol]</td>\n",
       "      <td>679.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>[australian, famili, guy, emerg, new, conserv,...</td>\n",
       "      <td>430.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>[australia, elect, outcom, incred, shock, lect...</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106443</th>\n",
       "      <td>[braddonvot, bassvot, lyonvot, abchobart, hoba...</td>\n",
       "      <td>574.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51759</th>\n",
       "      <td>[dear, white, cishet, babi, boomer, racist, ra...</td>\n",
       "      <td>897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129619</th>\n",
       "      <td>[oh, good, graciou, cinema, neg, ad, bill, lnp...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172957</th>\n",
       "      <td>[guardianau, anti, lnp, propaganda, guy, ausvot]</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68039</th>\n",
       "      <td>[mp, warren, entsch, concern, young, girl, see...</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177246</th>\n",
       "      <td>[jim, molan, rogu, senat, campaign, label, sab...</td>\n",
       "      <td>338.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167908</th>\n",
       "      <td>[happpyelectionday, ausvot, democracysausag, h...</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87844</th>\n",
       "      <td>[vale, bob, australian, love, rememb, solidar,...</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173260</th>\n",
       "      <td>[4, 05m, vote, cast, earli, vote, centr, cob, ...</td>\n",
       "      <td>1356.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68587</th>\n",
       "      <td>[anyway, sorri, guy, elect, night, im, gonna, ...</td>\n",
       "      <td>549.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144604</th>\n",
       "      <td>[mani, candid, pull, earli, elect, never, pre,...</td>\n",
       "      <td>1434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179149</th>\n",
       "      <td>[today, torr, strait, island, lodg, complaint,...</td>\n",
       "      <td>1413.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>[lilydambrosiomp, seem, rural, seat, well, one...</td>\n",
       "      <td>596.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157006</th>\n",
       "      <td>[today, believ, au, opinion, poll, say, differ...</td>\n",
       "      <td>709.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97288</th>\n",
       "      <td>[3, elect, 3, year, amp, imagin, 1975, murdoch...</td>\n",
       "      <td>421.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42315</th>\n",
       "      <td>[sum, auspol, ausvot, especi, clivepalm, amp, ...</td>\n",
       "      <td>343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>[trade, australia, morrison, secur, major, sha...</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156246</th>\n",
       "      <td>[antonygreen, point, poll, methodolog, chang, ...</td>\n",
       "      <td>727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61041</th>\n",
       "      <td>[whelp, look, like, anoth, 3, year, exil, aust...</td>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64341</th>\n",
       "      <td>[easi, tonyabbottmhr, rather, loser, quitter, ...</td>\n",
       "      <td>637.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66878</th>\n",
       "      <td>[want, labor, minor, gov, ff, ride, home, folk...</td>\n",
       "      <td>593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169071</th>\n",
       "      <td>[happi, vote, ausvot]</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57932</th>\n",
       "      <td>[gobsmack, australia, absolut, cook, auspol]</td>\n",
       "      <td>734.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30532</th>\n",
       "      <td>[joeltooley, describ, report, lindseybev, via,...</td>\n",
       "      <td>633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68529</th>\n",
       "      <td>[oz, fckd, ausvot, auspol, http, co, p0krqmxuwd]</td>\n",
       "      <td>552.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153637</th>\n",
       "      <td>[billshortenmp, conced, elect, recontest, lead...</td>\n",
       "      <td>819.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61016</th>\n",
       "      <td>[mayb, consol, one, bad, one, lose, let, see, ...</td>\n",
       "      <td>689.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154108</th>\n",
       "      <td>[propos, new, unit, state, australia, ausvot, ...</td>\n",
       "      <td>802.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64692</th>\n",
       "      <td>[tell, ya, letter, mum, got, target, elderli, ...</td>\n",
       "      <td>633.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166713</th>\n",
       "      <td>[aapmick, peter, dutton, say, organis, send, l...</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16235</th>\n",
       "      <td>[feder, elect, join, 7new, anchor, michaelush,...</td>\n",
       "      <td>330.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53550</th>\n",
       "      <td>[interest, see, malcolm, turnbul, respons, rea...</td>\n",
       "      <td>832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98838</th>\n",
       "      <td>[bernardkean, swannyqld, turn, abc24, cheer, v...</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24676</th>\n",
       "      <td>[vote, paulin, hanson, one, nation, want, vote...</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95467</th>\n",
       "      <td>[aussie_oliv, inde, abc730, w, scottmorrisonmp...</td>\n",
       "      <td>579.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154123</th>\n",
       "      <td>[bill, shorten, arriv, australianlabor, headqu...</td>\n",
       "      <td>801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40749</th>\n",
       "      <td>[quick, look, poll, place, level, 2pp, result,...</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20680</th>\n",
       "      <td>[ask, australian, elect, said, australia, need...</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111892</th>\n",
       "      <td>[liberalau, scottmorrisonmp, joshfrydenberg, a...</td>\n",
       "      <td>1436.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65372</th>\n",
       "      <td>[one, thing, certain, way, track, poll, eg, ne...</td>\n",
       "      <td>623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80363</th>\n",
       "      <td>[join, birdlifeoz, call, elect, need, stronger...</td>\n",
       "      <td>1258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99512</th>\n",
       "      <td>[funni, clive, palmer, spent, 20, million, get...</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109705</th>\n",
       "      <td>[baxterpeterba, right, high, commiss, london, ...</td>\n",
       "      <td>223.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99719</th>\n",
       "      <td>[zalisteggal, acoss, worri, zalisteggal, fix, ...</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75830</th>\n",
       "      <td>[front, gate, moone, pond, west, p, billshorte...</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15119</th>\n",
       "      <td>[top, stori, de, brandmarkespana, australian, ...</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46147</th>\n",
       "      <td>[phillipadams_1, lie, corrupt, paralysi, disun...</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81474</th>\n",
       "      <td>[auspol, obvious, pas, bob, hawk, put, lnp, qu...</td>\n",
       "      <td>782.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102307</th>\n",
       "      <td>[auspol, australiavotes2019, http, co, 2cm8me3...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75410</th>\n",
       "      <td>[vote, hand, volunt, mani, candid, one, messag...</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30253</th>\n",
       "      <td>[c, mon, debfrecklington, lnp, colleagu, conde...</td>\n",
       "      <td>670.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23168</th>\n",
       "      <td>[mecardoanalysi, elect, reach, fever, pitch, t...</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146703 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0       1\n",
       "35111   [bright, side, cancer, chariti, expect, influx...   118.0\n",
       "179673  [abctv, abccompass, watch, kumi, taguchi, late...   568.0\n",
       "103060                     [auspol, http, co, btpyxpfx7n]  1376.0\n",
       "62430   [look, like, hung, parliament, mojonewsau, aus...   668.0\n",
       "176772  [worst, part, elect, tri, figur, put, last, ua...   799.0\n",
       "39349          [smh, jacquelinemaley, politician, auspol]   679.0\n",
       "4598    [australian, famili, guy, emerg, new, conserv,...   430.0\n",
       "1752    [australia, elect, outcom, incred, shock, lect...    11.0\n",
       "106443  [braddonvot, bassvot, lyonvot, abchobart, hoba...   574.0\n",
       "51759   [dear, white, cishet, babi, boomer, racist, ra...   897.0\n",
       "129619  [oh, good, graciou, cinema, neg, ad, bill, lnp...    13.0\n",
       "172957   [guardianau, anti, lnp, propaganda, guy, ausvot]   109.0\n",
       "68039   [mp, warren, entsch, concern, young, girl, see...   570.0\n",
       "177246  [jim, molan, rogu, senat, campaign, label, sab...   338.0\n",
       "167908  [happpyelectionday, ausvot, democracysausag, h...   217.0\n",
       "87844   [vale, bob, australian, love, rememb, solidar,...   119.0\n",
       "173260  [4, 05m, vote, cast, earli, vote, centr, cob, ...  1356.0\n",
       "68587   [anyway, sorri, guy, elect, night, im, gonna, ...   549.0\n",
       "144604  [mani, candid, pull, earli, elect, never, pre,...  1434.0\n",
       "179149  [today, torr, strait, island, lodg, complaint,...  1413.0\n",
       "39990   [lilydambrosiomp, seem, rural, seat, well, one...   596.0\n",
       "157006  [today, believ, au, opinion, poll, say, differ...   709.0\n",
       "97288   [3, elect, 3, year, amp, imagin, 1975, murdoch...   421.0\n",
       "42315   [sum, auspol, ausvot, especi, clivepalm, amp, ...   343.0\n",
       "573     [trade, australia, morrison, secur, major, sha...   394.0\n",
       "156246  [antonygreen, point, poll, methodolog, chang, ...   727.0\n",
       "61041   [whelp, look, like, anoth, 3, year, exil, aust...   689.0\n",
       "64341   [easi, tonyabbottmhr, rather, loser, quitter, ...   637.0\n",
       "66878   [want, labor, minor, gov, ff, ride, home, folk...   593.0\n",
       "169071                              [happi, vote, ausvot]   103.0\n",
       "...                                                   ...     ...\n",
       "57932        [gobsmack, australia, absolut, cook, auspol]   734.0\n",
       "30532   [joeltooley, describ, report, lindseybev, via,...   633.0\n",
       "68529    [oz, fckd, ausvot, auspol, http, co, p0krqmxuwd]   552.0\n",
       "153637  [billshortenmp, conced, elect, recontest, lead...   819.0\n",
       "61016   [mayb, consol, one, bad, one, lose, let, see, ...   689.0\n",
       "154108  [propos, new, unit, state, australia, ausvot, ...   802.0\n",
       "64692   [tell, ya, letter, mum, got, target, elderli, ...   633.0\n",
       "166713  [aapmick, peter, dutton, say, organis, send, l...   366.0\n",
       "16235   [feder, elect, join, 7new, anchor, michaelush,...   330.0\n",
       "53550   [interest, see, malcolm, turnbul, respons, rea...   832.0\n",
       "98838   [bernardkean, swannyqld, turn, abc24, cheer, v...   281.0\n",
       "24676   [vote, paulin, hanson, one, nation, want, vote...   242.0\n",
       "95467   [aussie_oliv, inde, abc730, w, scottmorrisonmp...   579.0\n",
       "154123  [bill, shorten, arriv, australianlabor, headqu...   801.0\n",
       "40749   [quick, look, poll, place, level, 2pp, result,...   512.0\n",
       "20680   [ask, australian, elect, said, australia, need...   117.0\n",
       "111892  [liberalau, scottmorrisonmp, joshfrydenberg, a...  1436.0\n",
       "65372   [one, thing, certain, way, track, poll, eg, ne...   623.0\n",
       "80363   [join, birdlifeoz, call, elect, need, stronger...  1258.0\n",
       "99512   [funni, clive, palmer, spent, 20, million, get...   235.0\n",
       "109705  [baxterpeterba, right, high, commiss, london, ...   223.0\n",
       "99719   [zalisteggal, acoss, worri, zalisteggal, fix, ...   220.0\n",
       "75830   [front, gate, moone, pond, west, p, billshorte...   103.0\n",
       "15119   [top, stori, de, brandmarkespana, australian, ...   560.0\n",
       "46147   [phillipadams_1, lie, corrupt, paralysi, disun...    55.0\n",
       "81474   [auspol, obvious, pas, bob, hawk, put, lnp, qu...   782.0\n",
       "102307  [auspol, australiavotes2019, http, co, 2cm8me3...     7.0\n",
       "75410   [vote, hand, volunt, mani, candid, one, messag...   126.0\n",
       "30253   [c, mon, debfrecklington, lnp, colleagu, conde...   670.0\n",
       "23168   [mecardoanalysi, elect, reach, fever, pitch, t...   137.0\n",
       "\n",
       "[146703 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datosfinal=pd.concat([result2,horasa,respuestas],axis=1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(datosfinal[[0,1]], datosfinal.retweet_count, test_size=0.2)\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado a que se tienen datos faltantes en el campo de hora, se crean variables nuevas de tal manera que los datos faltantes tanto en los datos de observación como en los datos de prueba correspondan a la media de los demás datos, esto con el fin de que los algorítmos sean capaces de procesar la información, y de acuerdo a las recomendaciones de varios autores del tema, que sugieren reemplazar por la media o la mediana.\n",
    "\n",
    "Las variables obtenidas de hora después de hacer este reemplazo corresponden a horastrain y horastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "horastrain=np.where(np.isnan(X_train[[1]]), ma.array(X_train[[1]], mask=np.isnan(X_train[[1]])).mean(axis=0), X_train[[1]]) \n",
    "horastest=np.where(np.isnan(X_test[[1]]), ma.array(X_test[[1]], mask=np.isnan(X_test[[1]])).mean(axis=0), X_test[[1]]) \n",
    "horastrain = horastrain.reshape(len(horastrain), 1)\n",
    "horastest = horastest.reshape(len(horastest), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para procesar la información de las listas de palabras de los textos de los tweets, se utilizará el algoritmo TF-IDF, el cual asigna a cada palabra un valor dependiendo del número de veces que aparece en el texto y la frecuencia de aparición en los textos. Es así que el valor TF-IDF por palabra será calculado como (Número de veces que la palabra aparece en el tweet)/(Número de palabras en el tweet)*ln(Número de documentos/número de tweets que contiene la palabra)\n",
    "\n",
    "Para realizar este procedimiento y cuantificar los textos de los tweets se utilizará la función TfidfVectorizer de sklearn, esta permite hallar el TF_IDF de todas las palabras de todos los tweets  tanto en los datos de entrenamiento como en los datos de prueba. Esto permite extraer el nivel de rareza de las palabras que presentacada tweet.\n",
    "\n",
    "En primer lugar se calcula el valor TF-IDF para los datos de entranamiento y se guarda en la matriz datos train, posteriormente se extrae el vocabulario usado en los datos de entrenamiento, con el cual se calculó el TF-IDF, para usar el mismo diccionario en los datos de prueba. Se considera que dado que los datos de entrenamiento son suficientemente amplios, la mayoria de las palabras de los datos de prueba estará en este diccionario. Este diccionario se guardará en la variable aa.\n",
    "\n",
    "Posteriormente se calcula el TF-IDF para los datos de prueba basados en las palabras de los datos de entrenamiento. El resultado se guardará en la variable datostest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def funciondummy(doc):\n",
    "    return doc\n",
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=funciondummy,\n",
    "    preprocessor=funciondummy,\n",
    "    token_pattern=None)\n",
    "Xftr=X_train[[0]].iloc[:,0]\n",
    "Xftt=X_test[[0]].iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta=tfidf.fit(Xftr)\n",
    "datostrain=respuesta.transform(Xftr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(146703, 147099)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=tfidf.vocabulary_.keys()\n",
    "print(len(aa))\n",
    "datostrain.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=funciondummy,\n",
    "    preprocessor=funciondummy,\n",
    "    token_pattern=None, vocabulary=aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta=tfidf.fit(Xftt)\n",
    "datostest=respuesta.transform(Xftt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146703, 147099)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(36676, 147099)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(datostrain.shape)\n",
    "datostest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CountVectorizer.get_feature_names of TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2',\n",
      "        preprocessor=<function funciondummy at 0x000002B7AE812950>,\n",
      "        smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "        sublinear_tf=False, token_pattern=None,\n",
      "        tokenizer=<function funciondummy at 0x000002B7AE812950>,\n",
      "        use_idf=True,\n",
      "        vocabulary=dict_keys(['bright', 'side', 'cancer', 'chariti', 'expect', 'influx', 'donat', 'frankingcredit', 'refund', 'need', 'coupl', 'billion', 'match', 'labor', 'plan', 'auspol', 'ausvot', 'abctv', 'abccompass', 'watch', 'kumi', 'taguchi', 'latest', 'present', 'brilliant', 'obviou', 'virtu', 'par...guym', 'anqe6m467d', 'quandari', '2cm8me3hpm', 'xo65gj2j5u', 'nibpdzseka', 'mecardoanalysi', '255']))>\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.get_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unen los valores obtenidos por TF-IDF con el parametro de la hora tanto para los datos de entrenamiento como para los de prueba. Dado que los resultados de TF-IDF son una sparse matrix, se utiliza la función hstack. \n",
    "La matriz resultante para los datos de entrenamiento es Xfinaltrain y para los de prueba Xfinaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as spar\n",
    "Xfinaltrain=spar.hstack((datostrain,horastrain))\n",
    "Xfinaltest=spar.hstack((datostest,horastest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de reducir la dimensionalidad de los datos de entrada y poder procesar la información con los resursos y tiempo disponibles, se usará el metodo TruncatedSVD, se intentó usar el método PCA, y estandarizar los datos, pero dado a que se están usando sparse matrix, esto no fue posible con un computador con los que se contaba.\n",
    "\n",
    "El metodo TruncatedSVD tiene la ventaja de que las entradas al mismo no deben estar estandarizadas para su uso y es utilizado frecuentemente con sparse matrix, a pesar de que la literatura de análisis de texto sugiere usar 100 dimensiones como salida, para este caso en particular se utilizarán únicamente 10, de manera que la información se pueda procesar más rapidamente cuando se esten implementando los algoritmos de entrenamiento. Los resultados se guardan en las matrices X_projtrain para los datos de entrenamiento y X_projtestpara los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "model = TruncatedSVD(n_components=10).fit(Xfinaltrain)\n",
    "X_projtrain = model.transform(Xfinaltrain)\n",
    "model = TruncatedSVD(n_components=10).fit(Xfinaltest)\n",
    "X_projtest = model.transform(Xfinaltest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera los resultados para los datos de entrenamiento X_projtrain y para los datos de prueba X_projtest, ya se encuentran listos para usar en un algoritmo de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51735     1\n",
       "123772    0\n",
       "165888    0\n",
       "15266     0\n",
       "134395    0\n",
       "96736     0\n",
       "115754    1\n",
       "178744    0\n",
       "127185    0\n",
       "144997    0\n",
       "120456    0\n",
       "158540    1\n",
       "120850    0\n",
       "118163    1\n",
       "82414     1\n",
       "28075     0\n",
       "2919      0\n",
       "123801    0\n",
       "166072    1\n",
       "75884     0\n",
       "3984      0\n",
       "163217    0\n",
       "36206     0\n",
       "48424     0\n",
       "28022     0\n",
       "93829     0\n",
       "13600     0\n",
       "75741     0\n",
       "36882     0\n",
       "94308     1\n",
       "         ..\n",
       "65464     1\n",
       "90881     0\n",
       "75786     0\n",
       "85570     0\n",
       "81808     0\n",
       "89624     1\n",
       "127492    1\n",
       "32051     1\n",
       "73732     0\n",
       "13125     0\n",
       "45875     0\n",
       "2739      0\n",
       "120546    0\n",
       "73026     1\n",
       "153986    0\n",
       "117757    1\n",
       "16508     0\n",
       "141911    0\n",
       "37367     0\n",
       "22159     0\n",
       "123503    0\n",
       "131306    1\n",
       "176720    0\n",
       "57990     0\n",
       "153175    0\n",
       "66810     1\n",
       "13705     0\n",
       "140182    0\n",
       "71600     1\n",
       "157116    0\n",
       "Name: retweet_count, Length: 36676, dtype: int16"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realizarán diferentes algoritmos de Machine Learning que permitan determinar la probabilidad de que un tweet sea retweeteado con base en su contenido y la hora en que fue publicado. Para realizar este procedimiento se tomaron en cuenta 3 algoritmos: Support Vector Machines, Redes Neuronales con capas ocultas y Adaboost con árboles de decisión como clasificador debil.\n",
    "\n",
    "El alcance del proyecto únicamente se limitó a estos 3 metodos, dada los buenos resultados que los mismos han obtenido en la literatura, y por falta de tiempo y recursos tecnológicos para realizar otros metodos, no obstante se tuvieron en consideracón: Random Forest, otros algorítmos de boosting como el de gradiente estocástico y modelos paramétricos como son el Logit y el Probit.\n",
    "\n",
    "En todos los modelos mostrados en este trabajo se realizó validación cruzada de k iteraciones que consiste en dividir los datos de entrenamiento en k muestras, tomar k-1 para entrenar el modelo y 1 para validarlo, y rotar los sets de elementos usados en cada tarea. Dadas las disposiciones tecnológicas y el tiempo, únicamente se tuvieron en cuenta 5 divisiones para la validación cruzada k=5. Para la validación cruzada en todos los metodos se utilizó la función cross_val_score.\n",
    "\n",
    "El primer modelo que se tomó como referencia fue Support Vector Machines, aunque inicialmente la idea era probar el algoritmo con diferentes tipos de kernel (lineal, polínomico, gaussiano), sin embargo por falta de tiempo únicamente se utilizó el kernel \"Radial Basis Function\", el cual tiene un hiperparametro adicional gamma.\n",
    "\n",
    "A continuación se presenta el modelo de entrenamiento de SVM, para calcular los hiperparametros C (el factor de regularización) y gamma, se realizo una pequeña Grid Search en una escala logaritmica para ambos parametros, para gamma se tomaron 6 valores entre 10e-2 y 10e3, y para C entre 10e-2 y 10e8. Se escogieron estos rangos dado a que se prefiere la simplicidad en los modelos y el algoritmo tarda bastante en su entrenamiento con la tecnología actual. Se limitó el número máximo de iteraciones a 50 para poder realizar el entrenamiento y el grid search rápidamente.\n",
    "\n",
    "Los resultados del nivel de clasificación por cada hiperparametro se guarda en la variabe lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_range = np.logspace(-2, 3, 6)\n",
    "C_range = np.logspace(-2, 8, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\anmam\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:244: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "lista=[]\n",
    "for c in C_range:\n",
    "    for gamma in gamma_range:\n",
    "        clf = svm.SVC(kernel='rbf', C=c, gamma=gamma, max_iter=500).fit(X_projtrain, y_train)\n",
    "        scores = cross_val_score(clf, X_projtest, y_test, cv=5)\n",
    "        lista.append((np.mean(scores),c,gamma))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6514888258097603, 0.01, 0.01),\n",
       " (0.6572690031753357, 0.01, 0.1),\n",
       " (0.659286722727567, 0.01, 1.0),\n",
       " (0.6583869123430806, 0.01, 10.0),\n",
       " (0.6566418915647522, 0.01, 100.0),\n",
       " (0.5948644490464269, 0.01, 1000.0),\n",
       " (0.6507254893320042, 1.0, 0.01),\n",
       " (0.6570781263115488, 1.0, 0.1),\n",
       " (0.6586868255856013, 1.0, 1.0),\n",
       " (0.6584959524406535, 1.0, 10.0),\n",
       " (0.6552514174577178, 1.0, 100.0),\n",
       " (0.5949462486475647, 1.0, 1000.0),\n",
       " (0.641618880723967, 100.0, 0.01),\n",
       " (0.6492802368334469, 100.0, 0.1),\n",
       " (0.6548151826916631, 100.0, 1.0),\n",
       " (0.6565874328382904, 100.0, 10.0),\n",
       " (0.6553604761413763, 100.0, 100.0),\n",
       " (0.5950654704646655, 100.0, 1000.0),\n",
       " (0.6391918397862006, 10000.0, 0.01),\n",
       " (0.6469624739229198, 10000.0, 0.1),\n",
       " (0.6563420340733928, 10000.0, 1.0),\n",
       " (0.65552410505686, 10000.0, 10.0),\n",
       " (0.6551696178646885, 10000.0, 100.0),\n",
       " (0.595147266338856, 10000.0, 1000.0),\n",
       " (0.6464987124355248, 1000000.0, 0.01),\n",
       " (0.6513250556057678, 1000000.0, 0.1),\n",
       " (0.6541879707231105, 1000000.0, 1.0),\n",
       " (0.40894230806762605, 1000000.0, 10.0),\n",
       " (0.6550878034023856, 1000000.0, 100.0),\n",
       " (0.5951472700556676, 1000000.0, 1000.0),\n",
       " (0.636002343388835, 100000000.0, 0.01),\n",
       " (0.649661280517214, 100000000.0, 0.1),\n",
       " (0.6540243715411066, 100000000.0, 1.0),\n",
       " (0.4721104068218251, 100000000.0, 10.0),\n",
       " (0.6551968843950225, 100000000.0, 100.0),\n",
       " (0.5951472700556676, 100000000.0, 1000.0)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el mejor resultado se obtiene cuando tanto gamma como c tiene un valor de 0.01 y gamma un valor de 1, con estos resultados podría realizarse nuevamente grid_search para ajustar de mejor manera el valor de los hiperparametros, pero dado al tiempo y teconología disponibles, esto se saldrá del alcance de este ejercicio. Cabe resaltar que estos resultados pueden estar equivocos dada la limitación del máximo número de iteraciones. El nivel máximo de exito alcanzado fue 65.92%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo modelo que se utilizó fue el modelo de redes neuronales con capas ocultas, para este caso únicamente se consideró hasta dos capas ocultas, ya que entre más complejo sea el modelo, más dificil la estimación y menos deseable para el análisis, además que se contaba con poco tiempo. \n",
    "El número de neuronas escogida como hiperparametro para cada una de las capas se encontró por medio de una pequeña grid search, en donde se tomaron valores entre 1 y 21  neuronas para la primera capa y entre 0 y 20 para la segunda, se limitó nuevamente el número máximo de iteraciones hasta 150. El numero de particiones para la validación cruzada continúa siendo 5.\n",
    "\n",
    "Los resultados de elnivel de clasificación en los datos de prueba con cada par de hiperparametros se guardan en la variable listarn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64107652\n",
      "Iteration 2, loss = 0.64094130\n",
      "Iteration 3, loss = 0.64097342\n",
      "Iteration 4, loss = 0.64094889\n",
      "Iteration 5, loss = 0.64084724\n",
      "Iteration 6, loss = 0.64086510\n",
      "Iteration 7, loss = 0.64608677\n",
      "Iteration 8, loss = 0.64813519\n",
      "Iteration 9, loss = 0.64799941\n",
      "Iteration 10, loss = 0.64808019\n",
      "Iteration 11, loss = 0.64813369\n",
      "Iteration 12, loss = 0.64809197\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64189611\n",
      "Iteration 2, loss = 0.64132938\n",
      "Iteration 3, loss = 0.64144143\n",
      "Iteration 4, loss = 0.64138102\n",
      "Iteration 5, loss = 0.64139897\n",
      "Iteration 6, loss = 0.64147340\n",
      "Iteration 7, loss = 0.64132093\n",
      "Iteration 8, loss = 0.64127771\n",
      "Iteration 9, loss = 0.64139732\n",
      "Iteration 10, loss = 0.64124245\n",
      "Iteration 11, loss = 0.64162450\n",
      "Iteration 12, loss = 0.64146569\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64187148\n",
      "Iteration 2, loss = 0.64141692\n",
      "Iteration 3, loss = 0.64145897\n",
      "Iteration 4, loss = 0.64142039\n",
      "Iteration 5, loss = 0.64138511\n",
      "Iteration 6, loss = 0.64145123\n",
      "Iteration 7, loss = 0.64137407\n",
      "Iteration 8, loss = 0.64146549\n",
      "Iteration 9, loss = 0.64136400\n",
      "Iteration 10, loss = 0.64133197\n",
      "Iteration 11, loss = 0.64160193\n",
      "Iteration 12, loss = 0.64141463\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64189992\n",
      "Iteration 2, loss = 0.64139982\n",
      "Iteration 3, loss = 0.64140859\n",
      "Iteration 4, loss = 0.64135422\n",
      "Iteration 5, loss = 0.64148125\n",
      "Iteration 6, loss = 0.64144302\n",
      "Iteration 7, loss = 0.64135554\n",
      "Iteration 8, loss = 0.64136633\n",
      "Iteration 9, loss = 0.64137809\n",
      "Iteration 10, loss = 0.64134408\n",
      "Iteration 11, loss = 0.64120373\n",
      "Iteration 12, loss = 0.64148256\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64201993\n",
      "Iteration 2, loss = 0.64148602\n",
      "Iteration 3, loss = 0.64137766\n",
      "Iteration 4, loss = 0.64138209\n",
      "Iteration 5, loss = 0.64144845\n",
      "Iteration 6, loss = 0.64141230\n",
      "Iteration 7, loss = 0.64133102\n",
      "Iteration 8, loss = 0.64143031\n",
      "Iteration 9, loss = 0.64135523\n",
      "Iteration 10, loss = 0.64134571\n",
      "Iteration 11, loss = 0.64135037\n",
      "Iteration 12, loss = 0.64134396\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64195456\n",
      "Iteration 2, loss = 0.64144789\n",
      "Iteration 3, loss = 0.64140089\n",
      "Iteration 4, loss = 0.64136371\n",
      "Iteration 5, loss = 0.64151352\n",
      "Iteration 6, loss = 0.64131655\n",
      "Iteration 7, loss = 0.64137248\n",
      "Iteration 8, loss = 0.64148683\n",
      "Iteration 9, loss = 0.64141990\n",
      "Iteration 10, loss = 0.64141228\n",
      "Iteration 11, loss = 0.64140981\n",
      "Iteration 12, loss = 0.64153262\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64110824\n",
      "Iteration 2, loss = 0.64104142\n",
      "Iteration 3, loss = 0.64094070\n",
      "Iteration 4, loss = 0.64101301\n",
      "Iteration 5, loss = 0.64090937\n",
      "Iteration 6, loss = 0.64094743\n",
      "Iteration 7, loss = 0.64098900\n",
      "Iteration 8, loss = 0.64095042\n",
      "Iteration 9, loss = 0.64087405\n",
      "Iteration 10, loss = 0.64101510\n",
      "Iteration 11, loss = 0.64089899\n",
      "Iteration 12, loss = 0.64098771\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64166487\n",
      "Iteration 2, loss = 0.64138291\n",
      "Iteration 3, loss = 0.64154070\n",
      "Iteration 4, loss = 0.64146965\n",
      "Iteration 5, loss = 0.64166147\n",
      "Iteration 6, loss = 0.64161201\n",
      "Iteration 7, loss = 0.64148649\n",
      "Iteration 8, loss = 0.64152186\n",
      "Iteration 9, loss = 0.64137873\n",
      "Iteration 10, loss = 0.64169709\n",
      "Iteration 11, loss = 0.64146247\n",
      "Iteration 12, loss = 0.64160880\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64170137\n",
      "Iteration 2, loss = 0.64137651\n",
      "Iteration 3, loss = 0.64154512\n",
      "Iteration 4, loss = 0.64139384\n",
      "Iteration 5, loss = 0.64172305\n",
      "Iteration 6, loss = 0.64154227\n",
      "Iteration 7, loss = 0.64146073\n",
      "Iteration 8, loss = 0.64165800\n",
      "Iteration 9, loss = 0.64132452\n",
      "Iteration 10, loss = 0.64160809\n",
      "Iteration 11, loss = 0.64145944\n",
      "Iteration 12, loss = 0.64160850\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64185684\n",
      "Iteration 2, loss = 0.64147719\n",
      "Iteration 3, loss = 0.64159088\n",
      "Iteration 4, loss = 0.64150293\n",
      "Iteration 5, loss = 0.64154389\n",
      "Iteration 6, loss = 0.64139370\n",
      "Iteration 7, loss = 0.64148046\n",
      "Iteration 8, loss = 0.64129911\n",
      "Iteration 9, loss = 0.64151051\n",
      "Iteration 10, loss = 0.64160702\n",
      "Iteration 11, loss = 0.64143706\n",
      "Iteration 12, loss = 0.64165809\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64184777\n",
      "Iteration 2, loss = 0.64160423\n",
      "Iteration 3, loss = 0.64158058\n",
      "Iteration 4, loss = 0.64149570\n",
      "Iteration 5, loss = 0.64155197\n",
      "Iteration 6, loss = 0.64146163\n",
      "Iteration 7, loss = 0.64146464\n",
      "Iteration 8, loss = 0.64137118\n",
      "Iteration 9, loss = 0.64165805\n",
      "Iteration 10, loss = 0.64157807\n",
      "Iteration 11, loss = 0.64155697\n",
      "Iteration 12, loss = 0.64168015\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64185692\n",
      "Iteration 2, loss = 0.64157170\n",
      "Iteration 3, loss = 0.64133281\n",
      "Iteration 4, loss = 0.64173950\n",
      "Iteration 5, loss = 0.64163433\n",
      "Iteration 6, loss = 0.64153653\n",
      "Iteration 7, loss = 0.64178012\n",
      "Iteration 8, loss = 0.64150050\n",
      "Iteration 9, loss = 0.64156382\n",
      "Iteration 10, loss = 0.64153525\n",
      "Iteration 11, loss = 0.64145846\n",
      "Iteration 12, loss = 0.64138377\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64192445\n",
      "Iteration 2, loss = 0.64108698\n",
      "Iteration 3, loss = 0.64096388\n",
      "Iteration 4, loss = 0.64099126\n",
      "Iteration 5, loss = 0.64092492\n",
      "Iteration 6, loss = 0.64093580\n",
      "Iteration 7, loss = 0.64087642\n",
      "Iteration 8, loss = 0.64094046\n",
      "Iteration 9, loss = 0.64095774\n",
      "Iteration 10, loss = 0.64094400\n",
      "Iteration 11, loss = 0.64093748\n",
      "Iteration 12, loss = 0.64092473\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64493958\n",
      "Iteration 2, loss = 0.64143517\n",
      "Iteration 3, loss = 0.64146225\n",
      "Iteration 4, loss = 0.64174558\n",
      "Iteration 5, loss = 0.64143404\n",
      "Iteration 6, loss = 0.64132495\n",
      "Iteration 7, loss = 0.64164450\n",
      "Iteration 8, loss = 0.64156940\n",
      "Iteration 9, loss = 0.64148066\n",
      "Iteration 10, loss = 0.64155356\n",
      "Iteration 11, loss = 0.64155996\n",
      "Iteration 12, loss = 0.64137510\n",
      "Iteration 13, loss = 0.64153601\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64462466\n",
      "Iteration 2, loss = 0.64140667\n",
      "Iteration 3, loss = 0.64140488\n",
      "Iteration 4, loss = 0.64181287\n",
      "Iteration 5, loss = 0.64147983\n",
      "Iteration 6, loss = 0.64146126\n",
      "Iteration 7, loss = 0.64148446\n",
      "Iteration 8, loss = 0.64158620\n",
      "Iteration 9, loss = 0.64149200\n",
      "Iteration 10, loss = 0.64165055\n",
      "Iteration 11, loss = 0.64172301\n",
      "Iteration 12, loss = 0.64140887\n",
      "Iteration 13, loss = 0.64141058\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64615744\n",
      "Iteration 2, loss = 0.64174089\n",
      "Iteration 3, loss = 0.64176400\n",
      "Iteration 4, loss = 0.64172444\n",
      "Iteration 5, loss = 0.64178278\n",
      "Iteration 6, loss = 0.64154769\n",
      "Iteration 7, loss = 0.64148509\n",
      "Iteration 8, loss = 0.64154578\n",
      "Iteration 9, loss = 0.64150137\n",
      "Iteration 10, loss = 0.64150171\n",
      "Iteration 11, loss = 0.64138879\n",
      "Iteration 12, loss = 0.64160379\n",
      "Iteration 13, loss = 0.64138980\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64605498\n",
      "Iteration 2, loss = 0.64158324\n",
      "Iteration 3, loss = 0.64152571\n",
      "Iteration 4, loss = 0.64168459\n",
      "Iteration 5, loss = 0.64167492\n",
      "Iteration 6, loss = 0.64155978\n",
      "Iteration 7, loss = 0.64161808\n",
      "Iteration 8, loss = 0.64166172\n",
      "Iteration 9, loss = 0.64141801\n",
      "Iteration 10, loss = 0.64133665\n",
      "Iteration 11, loss = 0.64148836\n",
      "Iteration 12, loss = 0.64163016\n",
      "Iteration 13, loss = 0.64138074\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64540168\n",
      "Iteration 2, loss = 0.64177829\n",
      "Iteration 3, loss = 0.64127463\n",
      "Iteration 4, loss = 0.64180362\n",
      "Iteration 5, loss = 0.64163093\n",
      "Iteration 6, loss = 0.64145170\n",
      "Iteration 7, loss = 0.64158646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 0.64141625\n",
      "Iteration 9, loss = 0.64148450\n",
      "Iteration 10, loss = 0.64151899\n",
      "Iteration 11, loss = 0.64146543\n",
      "Iteration 12, loss = 0.64152428\n",
      "Iteration 13, loss = 0.64154152\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64140263\n",
      "Iteration 2, loss = 0.64114274\n",
      "Iteration 3, loss = 0.64112250\n",
      "Iteration 4, loss = 0.64105361\n",
      "Iteration 5, loss = 0.64095236\n",
      "Iteration 6, loss = 0.64097194\n",
      "Iteration 7, loss = 0.64097827\n",
      "Iteration 8, loss = 0.64087638\n",
      "Iteration 9, loss = 0.64087841\n",
      "Iteration 10, loss = 0.64100570\n",
      "Iteration 11, loss = 0.64091057\n",
      "Iteration 12, loss = 0.64089233\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64176407\n",
      "Iteration 2, loss = 0.64168295\n",
      "Iteration 3, loss = 0.64160554\n",
      "Iteration 4, loss = 0.64170127\n",
      "Iteration 5, loss = 0.64164354\n",
      "Iteration 6, loss = 0.64181399\n",
      "Iteration 7, loss = 0.64167994\n",
      "Iteration 8, loss = 0.64161850\n",
      "Iteration 9, loss = 0.64145894\n",
      "Iteration 10, loss = 0.64181036\n",
      "Iteration 11, loss = 0.64154744\n",
      "Iteration 12, loss = 0.64171359\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64208386\n",
      "Iteration 2, loss = 0.64175528\n",
      "Iteration 3, loss = 0.64184809\n",
      "Iteration 4, loss = 0.64158519\n",
      "Iteration 5, loss = 0.64167888\n",
      "Iteration 6, loss = 0.64161991\n",
      "Iteration 7, loss = 0.64164155\n",
      "Iteration 8, loss = 0.64174539\n",
      "Iteration 9, loss = 0.64152536\n",
      "Iteration 10, loss = 0.64154276\n",
      "Iteration 11, loss = 0.64184679\n",
      "Iteration 12, loss = 0.64171597\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64186140\n",
      "Iteration 2, loss = 0.64145696\n",
      "Iteration 3, loss = 0.64192198\n",
      "Iteration 4, loss = 0.64180316\n",
      "Iteration 5, loss = 0.64187433\n",
      "Iteration 6, loss = 0.64205340\n",
      "Iteration 7, loss = 0.64159274\n",
      "Iteration 8, loss = 0.64140749\n",
      "Iteration 9, loss = 0.64170104\n",
      "Iteration 10, loss = 0.64148337\n",
      "Iteration 11, loss = 0.64177763\n",
      "Iteration 12, loss = 0.64171881\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64182361\n",
      "Iteration 2, loss = 0.64154990\n",
      "Iteration 3, loss = 0.64176111\n",
      "Iteration 4, loss = 0.64172377\n",
      "Iteration 5, loss = 0.64213905\n",
      "Iteration 6, loss = 0.64197832\n",
      "Iteration 7, loss = 0.64171564\n",
      "Iteration 8, loss = 0.64154380\n",
      "Iteration 9, loss = 0.64178457\n",
      "Iteration 10, loss = 0.64142323\n",
      "Iteration 11, loss = 0.64174144\n",
      "Iteration 12, loss = 0.64151419\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64176568\n",
      "Iteration 2, loss = 0.64164395\n",
      "Iteration 3, loss = 0.64158775\n",
      "Iteration 4, loss = 0.64168055\n",
      "Iteration 5, loss = 0.64180972\n",
      "Iteration 6, loss = 0.64161112\n",
      "Iteration 7, loss = 0.64161336\n",
      "Iteration 8, loss = 0.64158867\n",
      "Iteration 9, loss = 0.64151809\n",
      "Iteration 10, loss = 0.64150456\n",
      "Iteration 11, loss = 0.64189618\n",
      "Iteration 12, loss = 0.64158450\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64117538\n",
      "Iteration 2, loss = 0.64106122\n",
      "Iteration 3, loss = 0.64110135\n",
      "Iteration 4, loss = 0.64094361\n",
      "Iteration 5, loss = 0.64100683\n",
      "Iteration 6, loss = 0.64103543\n",
      "Iteration 7, loss = 0.64095610\n",
      "Iteration 8, loss = 0.64090763\n",
      "Iteration 9, loss = 0.64089386\n",
      "Iteration 10, loss = 0.64087659\n",
      "Iteration 11, loss = 0.64093048\n",
      "Iteration 12, loss = 0.64089458\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64173843\n",
      "Iteration 2, loss = 0.64180840\n",
      "Iteration 3, loss = 0.64185082\n",
      "Iteration 4, loss = 0.64162993\n",
      "Iteration 5, loss = 0.64163950\n",
      "Iteration 6, loss = 0.64154108\n",
      "Iteration 7, loss = 0.64124051\n",
      "Iteration 8, loss = 0.64159622\n",
      "Iteration 9, loss = 0.64147626\n",
      "Iteration 10, loss = 0.64151239\n",
      "Iteration 11, loss = 0.64162287\n",
      "Iteration 12, loss = 0.64146969\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64175360\n",
      "Iteration 2, loss = 0.64168360\n",
      "Iteration 3, loss = 0.64173602\n",
      "Iteration 4, loss = 0.64139142\n",
      "Iteration 5, loss = 0.64159788\n",
      "Iteration 6, loss = 0.64163026\n",
      "Iteration 7, loss = 0.64140925\n",
      "Iteration 8, loss = 0.64181993\n",
      "Iteration 9, loss = 0.64148122\n",
      "Iteration 10, loss = 0.64155546\n",
      "Iteration 11, loss = 0.64150146\n",
      "Iteration 12, loss = 0.64156293\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64154325\n",
      "Iteration 2, loss = 0.64165788\n",
      "Iteration 3, loss = 0.64170502\n",
      "Iteration 4, loss = 0.64154575\n",
      "Iteration 5, loss = 0.64162120\n",
      "Iteration 6, loss = 0.64149875\n",
      "Iteration 7, loss = 0.64136365\n",
      "Iteration 8, loss = 0.64154600\n",
      "Iteration 9, loss = 0.64154820\n",
      "Iteration 10, loss = 0.64155697\n",
      "Iteration 11, loss = 0.64137726\n",
      "Iteration 12, loss = 0.64131170\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64164407\n",
      "Iteration 2, loss = 0.64166277\n",
      "Iteration 3, loss = 0.64177869\n",
      "Iteration 4, loss = 0.64152893\n",
      "Iteration 5, loss = 0.64153054\n",
      "Iteration 6, loss = 0.64151786\n",
      "Iteration 7, loss = 0.64150286\n",
      "Iteration 8, loss = 0.64160146\n",
      "Iteration 9, loss = 0.64157886\n",
      "Iteration 10, loss = 0.64171089\n",
      "Iteration 11, loss = 0.64145552\n",
      "Iteration 12, loss = 0.64145046\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64177763\n",
      "Iteration 2, loss = 0.64152061\n",
      "Iteration 3, loss = 0.64149674\n",
      "Iteration 4, loss = 0.64155187\n",
      "Iteration 5, loss = 0.64155420\n",
      "Iteration 6, loss = 0.64167242\n",
      "Iteration 7, loss = 0.64158152\n",
      "Iteration 8, loss = 0.64160403\n",
      "Iteration 9, loss = 0.64151555\n",
      "Iteration 10, loss = 0.64143396\n",
      "Iteration 11, loss = 0.64157407\n",
      "Iteration 12, loss = 0.64162526\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64850435\n",
      "Iteration 2, loss = 0.64489498\n",
      "Iteration 3, loss = 0.64494132\n",
      "Iteration 4, loss = 0.64484292\n",
      "Iteration 5, loss = 0.64491021\n",
      "Iteration 6, loss = 0.64495872\n",
      "Iteration 7, loss = 0.64488447\n",
      "Iteration 8, loss = 0.64483578\n",
      "Iteration 9, loss = 0.64481983\n",
      "Iteration 10, loss = 0.64480083\n",
      "Iteration 11, loss = 0.64485183\n",
      "Iteration 12, loss = 0.64481281\n",
      "Iteration 13, loss = 0.64478017\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66282532\n",
      "Iteration 2, loss = 0.64515753\n",
      "Iteration 3, loss = 0.64515869\n",
      "Iteration 4, loss = 0.64514739\n",
      "Iteration 5, loss = 0.64512094\n",
      "Iteration 6, loss = 0.64521386\n",
      "Iteration 7, loss = 0.64509314\n",
      "Iteration 8, loss = 0.64515106\n",
      "Iteration 9, loss = 0.64507836\n",
      "Iteration 10, loss = 0.64536372\n",
      "Iteration 11, loss = 0.64507589\n",
      "Iteration 12, loss = 0.64511607\n",
      "Iteration 13, loss = 0.64518138\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66117526\n",
      "Iteration 2, loss = 0.64472008\n",
      "Iteration 3, loss = 0.64475750\n",
      "Iteration 4, loss = 0.64465926\n",
      "Iteration 5, loss = 0.64472458\n",
      "Iteration 6, loss = 0.64484353\n",
      "Iteration 7, loss = 0.64477507\n",
      "Iteration 8, loss = 0.64481962\n",
      "Iteration 9, loss = 0.64467078\n",
      "Iteration 10, loss = 0.64474668\n",
      "Iteration 11, loss = 0.64468060\n",
      "Iteration 12, loss = 0.64467748\n",
      "Iteration 13, loss = 0.64472583\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66554683\n",
      "Iteration 2, loss = 0.64591092\n",
      "Iteration 3, loss = 0.64574724\n",
      "Iteration 4, loss = 0.64588018\n",
      "Iteration 5, loss = 0.64609311\n",
      "Iteration 6, loss = 0.64579998\n",
      "Iteration 7, loss = 0.64570511\n",
      "Iteration 8, loss = 0.64570928\n",
      "Iteration 9, loss = 0.64584007\n",
      "Iteration 10, loss = 0.64586811\n",
      "Iteration 11, loss = 0.64568202\n",
      "Iteration 12, loss = 0.64559312\n",
      "Iteration 13, loss = 0.64580104\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66191759\n",
      "Iteration 2, loss = 0.64486543\n",
      "Iteration 3, loss = 0.64485513\n",
      "Iteration 4, loss = 0.64481132\n",
      "Iteration 5, loss = 0.64499656\n",
      "Iteration 6, loss = 0.64472709\n",
      "Iteration 7, loss = 0.64482818\n",
      "Iteration 8, loss = 0.64475417\n",
      "Iteration 9, loss = 0.64484156\n",
      "Iteration 10, loss = 0.64491317\n",
      "Iteration 11, loss = 0.64473526\n",
      "Iteration 12, loss = 0.64472409\n",
      "Iteration 13, loss = 0.64478974\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.65890923\n",
      "Iteration 2, loss = 0.64418172\n",
      "Iteration 3, loss = 0.64431758\n",
      "Iteration 4, loss = 0.64404366\n",
      "Iteration 5, loss = 0.64416131\n",
      "Iteration 6, loss = 0.64417918\n",
      "Iteration 7, loss = 0.64406349\n",
      "Iteration 8, loss = 0.64406346\n",
      "Iteration 9, loss = 0.64407896\n",
      "Iteration 10, loss = 0.64414947\n",
      "Iteration 11, loss = 0.64404862\n",
      "Iteration 12, loss = 0.64424023\n",
      "Iteration 13, loss = 0.64424836\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64815892\n",
      "Iteration 2, loss = 0.64149880\n",
      "Iteration 3, loss = 0.64131918\n",
      "Iteration 4, loss = 0.64142296\n",
      "Iteration 5, loss = 0.64143095\n",
      "Iteration 6, loss = 0.64135967\n",
      "Iteration 7, loss = 0.64136620\n",
      "Iteration 8, loss = 0.64135104\n",
      "Iteration 9, loss = 0.64128964\n",
      "Iteration 10, loss = 0.64131326\n",
      "Iteration 11, loss = 0.64137592\n",
      "Iteration 12, loss = 0.64130142\n",
      "Iteration 13, loss = 0.64141057\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67587527\n",
      "Iteration 2, loss = 0.64186556\n",
      "Iteration 3, loss = 0.64207077\n",
      "Iteration 4, loss = 0.64176589\n",
      "Iteration 5, loss = 0.64178662\n",
      "Iteration 6, loss = 0.64184873\n",
      "Iteration 7, loss = 0.64195660\n",
      "Iteration 8, loss = 0.64189152\n",
      "Iteration 9, loss = 0.64178233\n",
      "Iteration 10, loss = 0.64197333\n",
      "Iteration 11, loss = 0.64188926\n",
      "Iteration 12, loss = 0.64174927\n",
      "Iteration 13, loss = 0.64183221\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67554394\n",
      "Iteration 2, loss = 0.64212784\n",
      "Iteration 3, loss = 0.64202088\n",
      "Iteration 4, loss = 0.64177407\n",
      "Iteration 5, loss = 0.64188763\n",
      "Iteration 6, loss = 0.64185264\n",
      "Iteration 7, loss = 0.64198204\n",
      "Iteration 8, loss = 0.64181541\n",
      "Iteration 9, loss = 0.64183535\n",
      "Iteration 10, loss = 0.64190219\n",
      "Iteration 11, loss = 0.64187532\n",
      "Iteration 12, loss = 0.64174902\n",
      "Iteration 13, loss = 0.64185505\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67771164\n",
      "Iteration 2, loss = 0.64193950\n",
      "Iteration 3, loss = 0.64216662\n",
      "Iteration 4, loss = 0.64199364\n",
      "Iteration 5, loss = 0.64217307\n",
      "Iteration 6, loss = 0.64204076\n",
      "Iteration 7, loss = 0.64231776\n",
      "Iteration 8, loss = 0.64193830\n",
      "Iteration 9, loss = 0.64193570\n",
      "Iteration 10, loss = 0.64187257\n",
      "Iteration 11, loss = 0.64216635\n",
      "Iteration 12, loss = 0.64201135\n",
      "Iteration 13, loss = 0.64203398\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67473467\n",
      "Iteration 2, loss = 0.64198991\n",
      "Iteration 3, loss = 0.64200513\n",
      "Iteration 4, loss = 0.64184313\n",
      "Iteration 5, loss = 0.64201533\n",
      "Iteration 6, loss = 0.64189253\n",
      "Iteration 7, loss = 0.64198231\n",
      "Iteration 8, loss = 0.64191988\n",
      "Iteration 9, loss = 0.64184739\n",
      "Iteration 10, loss = 0.64177062\n",
      "Iteration 11, loss = 0.64197646\n",
      "Iteration 12, loss = 0.64188216\n",
      "Iteration 13, loss = 0.64198431\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67257907\n",
      "Iteration 2, loss = 0.64207728\n",
      "Iteration 3, loss = 0.64174930\n",
      "Iteration 4, loss = 0.64173492\n",
      "Iteration 5, loss = 0.64192687\n",
      "Iteration 6, loss = 0.64175112\n",
      "Iteration 7, loss = 0.64184468\n",
      "Iteration 8, loss = 0.64162793\n",
      "Iteration 9, loss = 0.64202377\n",
      "Iteration 10, loss = 0.64179074\n",
      "Iteration 11, loss = 0.64175261\n",
      "Iteration 12, loss = 0.64173591\n",
      "Iteration 13, loss = 0.64174877\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64496885\n",
      "Iteration 2, loss = 0.64329793\n",
      "Iteration 3, loss = 0.64292547\n",
      "Iteration 4, loss = 0.64244321\n",
      "Iteration 5, loss = 0.64174760\n",
      "Iteration 6, loss = 0.64178095\n",
      "Iteration 7, loss = 0.64176834\n",
      "Iteration 8, loss = 0.64180403\n",
      "Iteration 9, loss = 0.64174729\n",
      "Iteration 10, loss = 0.64177630\n",
      "Iteration 11, loss = 0.64171968\n",
      "Iteration 12, loss = 0.64177202\n",
      "Iteration 13, loss = 0.64174437\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65484695\n",
      "Iteration 2, loss = 0.64387762\n",
      "Iteration 3, loss = 0.64376791\n",
      "Iteration 4, loss = 0.64360475\n",
      "Iteration 5, loss = 0.64374091\n",
      "Iteration 6, loss = 0.64371374\n",
      "Iteration 7, loss = 0.64377273\n",
      "Iteration 8, loss = 0.64378841\n",
      "Iteration 9, loss = 0.64370624\n",
      "Iteration 10, loss = 0.64342946\n",
      "Iteration 11, loss = 0.64371992\n",
      "Iteration 12, loss = 0.64379099\n",
      "Iteration 13, loss = 0.64388909\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65515013\n",
      "Iteration 2, loss = 0.64411350\n",
      "Iteration 3, loss = 0.64386605\n",
      "Iteration 4, loss = 0.64386179\n",
      "Iteration 5, loss = 0.64383597\n",
      "Iteration 6, loss = 0.64375243\n",
      "Iteration 7, loss = 0.64390811\n",
      "Iteration 8, loss = 0.64384846\n",
      "Iteration 9, loss = 0.64369974\n",
      "Iteration 10, loss = 0.64363732\n",
      "Iteration 11, loss = 0.64388499\n",
      "Iteration 12, loss = 0.64383661\n",
      "Iteration 13, loss = 0.64386803\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65542098\n",
      "Iteration 2, loss = 0.64441037\n",
      "Iteration 3, loss = 0.64417498\n",
      "Iteration 4, loss = 0.64428015\n",
      "Iteration 5, loss = 0.64414740\n",
      "Iteration 6, loss = 0.64408246\n",
      "Iteration 7, loss = 0.64412438\n",
      "Iteration 8, loss = 0.64424675\n",
      "Iteration 9, loss = 0.64429796\n",
      "Iteration 10, loss = 0.64401321\n",
      "Iteration 11, loss = 0.64424955\n",
      "Iteration 12, loss = 0.64405253\n",
      "Iteration 13, loss = 0.64410914\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65784226\n",
      "Iteration 2, loss = 0.64354843\n",
      "Iteration 3, loss = 0.64369339\n",
      "Iteration 4, loss = 0.64359986\n",
      "Iteration 5, loss = 0.64363942\n",
      "Iteration 6, loss = 0.64350132\n",
      "Iteration 7, loss = 0.64355582\n",
      "Iteration 8, loss = 0.64369983\n",
      "Iteration 9, loss = 0.64359314\n",
      "Iteration 10, loss = 0.64353544\n",
      "Iteration 11, loss = 0.64376637\n",
      "Iteration 12, loss = 0.64359466\n",
      "Iteration 13, loss = 0.64376005\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65809929\n",
      "Iteration 2, loss = 0.64357239\n",
      "Iteration 3, loss = 0.64357407\n",
      "Iteration 4, loss = 0.64364469\n",
      "Iteration 5, loss = 0.64338268\n",
      "Iteration 6, loss = 0.64366650\n",
      "Iteration 7, loss = 0.64353116\n",
      "Iteration 8, loss = 0.64341269\n",
      "Iteration 9, loss = 0.64363153\n",
      "Iteration 10, loss = 0.64350102\n",
      "Iteration 11, loss = 0.64370382\n",
      "Iteration 12, loss = 0.64345314\n",
      "Iteration 13, loss = 0.64368678\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65458588\n",
      "Iteration 2, loss = 0.64437216\n",
      "Iteration 3, loss = 0.64432516\n",
      "Iteration 4, loss = 0.64433147\n",
      "Iteration 5, loss = 0.64427489\n",
      "Iteration 6, loss = 0.64423475\n",
      "Iteration 7, loss = 0.64426540\n",
      "Iteration 8, loss = 0.64427721\n",
      "Iteration 9, loss = 0.64424799\n",
      "Iteration 10, loss = 0.64426310\n",
      "Iteration 11, loss = 0.64421868\n",
      "Iteration 12, loss = 0.64429939\n",
      "Iteration 13, loss = 0.64421893\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68812355\n",
      "Iteration 2, loss = 0.64424686\n",
      "Iteration 3, loss = 0.64415901\n",
      "Iteration 4, loss = 0.64421398\n",
      "Iteration 5, loss = 0.64422776\n",
      "Iteration 6, loss = 0.64480284\n",
      "Iteration 7, loss = 0.64408727\n",
      "Iteration 8, loss = 0.64424870\n",
      "Iteration 9, loss = 0.64430695\n",
      "Iteration 10, loss = 0.64406625\n",
      "Iteration 11, loss = 0.64415643\n",
      "Iteration 12, loss = 0.64424007\n",
      "Iteration 13, loss = 0.64418099\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68921008\n",
      "Iteration 2, loss = 0.64442927\n",
      "Iteration 3, loss = 0.64445134\n",
      "Iteration 4, loss = 0.64455753\n",
      "Iteration 5, loss = 0.64452006\n",
      "Iteration 6, loss = 0.64441616\n",
      "Iteration 7, loss = 0.64436728\n",
      "Iteration 8, loss = 0.64465128\n",
      "Iteration 9, loss = 0.64468061\n",
      "Iteration 10, loss = 0.64451107\n",
      "Iteration 11, loss = 0.64445556\n",
      "Iteration 12, loss = 0.64456066\n",
      "Iteration 13, loss = 0.64444418\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67838582\n",
      "Iteration 2, loss = 0.64305362\n",
      "Iteration 3, loss = 0.64326070\n",
      "Iteration 4, loss = 0.64312776\n",
      "Iteration 5, loss = 0.64309074\n",
      "Iteration 6, loss = 0.64296760\n",
      "Iteration 7, loss = 0.64289053\n",
      "Iteration 8, loss = 0.64303360\n",
      "Iteration 9, loss = 0.64298749\n",
      "Iteration 10, loss = 0.64291825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, loss = 0.64320541\n",
      "Iteration 12, loss = 0.64291079\n",
      "Iteration 13, loss = 0.64319550\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67866811\n",
      "Iteration 2, loss = 0.64302992\n",
      "Iteration 3, loss = 0.64308357\n",
      "Iteration 4, loss = 0.64288229\n",
      "Iteration 5, loss = 0.64298504\n",
      "Iteration 6, loss = 0.64302436\n",
      "Iteration 7, loss = 0.64291326\n",
      "Iteration 8, loss = 0.64296152\n",
      "Iteration 9, loss = 0.64293518\n",
      "Iteration 10, loss = 0.64282603\n",
      "Iteration 11, loss = 0.64310078\n",
      "Iteration 12, loss = 0.64287396\n",
      "Iteration 13, loss = 0.64312412\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69218901\n",
      "Iteration 2, loss = 0.64437090\n",
      "Iteration 3, loss = 0.64452347\n",
      "Iteration 4, loss = 0.64451568\n",
      "Iteration 5, loss = 0.64421745\n",
      "Iteration 6, loss = 0.64460253\n",
      "Iteration 7, loss = 0.64438554\n",
      "Iteration 8, loss = 0.64440405\n",
      "Iteration 9, loss = 0.64446935\n",
      "Iteration 10, loss = 0.64439828\n",
      "Iteration 11, loss = 0.64442936\n",
      "Iteration 12, loss = 0.64437418\n",
      "Iteration 13, loss = 0.64435713\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64995445\n",
      "Iteration 2, loss = 0.64353880\n",
      "Iteration 3, loss = 0.64358165\n",
      "Iteration 4, loss = 0.64347022\n",
      "Iteration 5, loss = 0.64349812\n",
      "Iteration 6, loss = 0.64339419\n",
      "Iteration 7, loss = 0.64340638\n",
      "Iteration 8, loss = 0.64338461\n",
      "Iteration 9, loss = 0.64350757\n",
      "Iteration 10, loss = 0.64346283\n",
      "Iteration 11, loss = 0.64343138\n",
      "Iteration 12, loss = 0.64355367\n",
      "Iteration 13, loss = 0.64340734\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67984996\n",
      "Iteration 2, loss = 0.64443262\n",
      "Iteration 3, loss = 0.64435862\n",
      "Iteration 4, loss = 0.64446110\n",
      "Iteration 5, loss = 0.64444632\n",
      "Iteration 6, loss = 0.64457642\n",
      "Iteration 7, loss = 0.64460185\n",
      "Iteration 8, loss = 0.64442090\n",
      "Iteration 9, loss = 0.64443282\n",
      "Iteration 10, loss = 0.64434841\n",
      "Iteration 11, loss = 0.64447053\n",
      "Iteration 12, loss = 0.64447356\n",
      "Iteration 13, loss = 0.64431024\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67731457\n",
      "Iteration 2, loss = 0.64426812\n",
      "Iteration 3, loss = 0.64400095\n",
      "Iteration 4, loss = 0.64415788\n",
      "Iteration 5, loss = 0.64422708\n",
      "Iteration 6, loss = 0.64433992\n",
      "Iteration 7, loss = 0.64420444\n",
      "Iteration 8, loss = 0.64419834\n",
      "Iteration 9, loss = 0.64398942\n",
      "Iteration 10, loss = 0.64396633\n",
      "Iteration 11, loss = 0.64420790\n",
      "Iteration 12, loss = 0.64423114\n",
      "Iteration 13, loss = 0.64394685\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66718957\n",
      "Iteration 2, loss = 0.64545663\n",
      "Iteration 3, loss = 0.64538390\n",
      "Iteration 4, loss = 0.64547411\n",
      "Iteration 5, loss = 0.64458842\n",
      "Iteration 6, loss = 0.64522271\n",
      "Iteration 7, loss = 0.64532023\n",
      "Iteration 8, loss = 0.64478431\n",
      "Iteration 9, loss = 0.64542010\n",
      "Iteration 10, loss = 0.64482163\n",
      "Iteration 11, loss = 0.64460702\n",
      "Iteration 12, loss = 0.64449854\n",
      "Iteration 13, loss = 0.64374330\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67551459\n",
      "Iteration 2, loss = 0.64410982\n",
      "Iteration 3, loss = 0.64408042\n",
      "Iteration 4, loss = 0.64456059\n",
      "Iteration 5, loss = 0.64454390\n",
      "Iteration 6, loss = 0.64401652\n",
      "Iteration 7, loss = 0.64406793\n",
      "Iteration 8, loss = 0.64411809\n",
      "Iteration 9, loss = 0.64421015\n",
      "Iteration 10, loss = 0.64393860\n",
      "Iteration 11, loss = 0.64410736\n",
      "Iteration 12, loss = 0.64425515\n",
      "Iteration 13, loss = 0.64412490\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67868511\n",
      "Iteration 2, loss = 0.64456836\n",
      "Iteration 3, loss = 0.64468717\n",
      "Iteration 4, loss = 0.64451065\n",
      "Iteration 5, loss = 0.64457838\n",
      "Iteration 6, loss = 0.64456897\n",
      "Iteration 7, loss = 0.64468682\n",
      "Iteration 8, loss = 0.64473045\n",
      "Iteration 9, loss = 0.64459199\n",
      "Iteration 10, loss = 0.64457740\n",
      "Iteration 11, loss = 0.64440132\n",
      "Iteration 12, loss = 0.64459226\n",
      "Iteration 13, loss = 0.64472900\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.91956530\n",
      "Iteration 2, loss = 1.91473484\n",
      "Iteration 3, loss = 1.91376648\n",
      "Iteration 4, loss = 1.91290343\n",
      "Iteration 5, loss = 1.91201261\n",
      "Iteration 6, loss = 1.91104003\n",
      "Iteration 7, loss = 1.91005370\n",
      "Iteration 8, loss = 1.90917067\n",
      "Iteration 9, loss = 1.90820448\n",
      "Iteration 10, loss = 1.90728144\n",
      "Iteration 11, loss = 1.90637784\n",
      "Iteration 12, loss = 1.90548056\n",
      "Iteration 13, loss = 1.91443798\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.69270880\n",
      "Iteration 2, loss = 1.65034227\n",
      "Iteration 3, loss = 1.65035910\n",
      "Iteration 4, loss = 1.65026634\n",
      "Iteration 5, loss = 1.65003387\n",
      "Iteration 6, loss = 1.64985937\n",
      "Iteration 7, loss = 1.64985254\n",
      "Iteration 8, loss = 1.64954834\n",
      "Iteration 9, loss = 1.64937829\n",
      "Iteration 10, loss = 1.64937193\n",
      "Iteration 11, loss = 1.64893401\n",
      "Iteration 12, loss = 1.64884852\n",
      "Iteration 13, loss = 1.64885739\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.52793141\n",
      "Iteration 2, loss = 1.47559236\n",
      "Iteration 3, loss = 1.47581122\n",
      "Iteration 4, loss = 1.47543660\n",
      "Iteration 5, loss = 1.47539645\n",
      "Iteration 6, loss = 1.47515418\n",
      "Iteration 7, loss = 1.47516204\n",
      "Iteration 8, loss = 1.47509124\n",
      "Iteration 9, loss = 1.47468642\n",
      "Iteration 10, loss = 1.47464008\n",
      "Iteration 11, loss = 1.47444224\n",
      "Iteration 12, loss = 1.47435749\n",
      "Iteration 13, loss = 1.47447173\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.67988555\n",
      "Iteration 2, loss = 1.63710375\n",
      "Iteration 3, loss = 1.63699224\n",
      "Iteration 4, loss = 1.63678065\n",
      "Iteration 5, loss = 1.63660376\n",
      "Iteration 6, loss = 1.63647514\n",
      "Iteration 7, loss = 1.63640829\n",
      "Iteration 8, loss = 1.63609611\n",
      "Iteration 9, loss = 1.63600170\n",
      "Iteration 10, loss = 1.63579537\n",
      "Iteration 11, loss = 1.63582937\n",
      "Iteration 12, loss = 1.63548478\n",
      "Iteration 13, loss = 1.63543666\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.41149274\n",
      "Iteration 2, loss = 1.34529783\n",
      "Iteration 3, loss = 1.34522316\n",
      "Iteration 4, loss = 1.34505681\n",
      "Iteration 5, loss = 1.34495657\n",
      "Iteration 6, loss = 1.34481836\n",
      "Iteration 7, loss = 1.34477411\n",
      "Iteration 8, loss = 1.34469579\n",
      "Iteration 9, loss = 1.34458422\n",
      "Iteration 10, loss = 1.34453229\n",
      "Iteration 11, loss = 1.34437563\n",
      "Iteration 12, loss = 1.34421699\n",
      "Iteration 13, loss = 1.34411876\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.55541673\n",
      "Iteration 2, loss = 1.49926728\n",
      "Iteration 3, loss = 1.49915162\n",
      "Iteration 4, loss = 1.49902890\n",
      "Iteration 5, loss = 1.49902826\n",
      "Iteration 6, loss = 1.49869582\n",
      "Iteration 7, loss = 1.49849075\n",
      "Iteration 8, loss = 1.49832277\n",
      "Iteration 9, loss = 1.49831169\n",
      "Iteration 10, loss = 1.49822852\n",
      "Iteration 11, loss = 1.49905004\n",
      "Iteration 12, loss = 1.50076086\n",
      "Iteration 13, loss = 1.50071114\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.72346120\n",
      "Iteration 2, loss = 1.71882022\n",
      "Iteration 3, loss = 1.71801897\n",
      "Iteration 4, loss = 1.71725536\n",
      "Iteration 5, loss = 1.71641766\n",
      "Iteration 6, loss = 1.71559194\n",
      "Iteration 7, loss = 1.71483902\n",
      "Iteration 8, loss = 1.71406460\n",
      "Iteration 9, loss = 1.71325037\n",
      "Iteration 10, loss = 1.71247972\n",
      "Iteration 11, loss = 1.71165110\n",
      "Iteration 12, loss = 1.71094789\n",
      "Iteration 13, loss = 1.71008375\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18384157\n",
      "Iteration 2, loss = 2.20955048\n",
      "Iteration 3, loss = 2.20919870\n",
      "Iteration 4, loss = 2.20907260\n",
      "Iteration 5, loss = 2.20880883\n",
      "Iteration 6, loss = 2.20867703\n",
      "Iteration 7, loss = 2.20824125\n",
      "Iteration 8, loss = 2.20812289\n",
      "Iteration 9, loss = 2.20797530\n",
      "Iteration 10, loss = 2.20756291\n",
      "Iteration 11, loss = 2.20739128\n",
      "Iteration 12, loss = 2.20721205\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.52697517\n",
      "Iteration 2, loss = 2.58185123\n",
      "Iteration 3, loss = 2.58153319\n",
      "Iteration 4, loss = 2.58136616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, loss = 2.58101651\n",
      "Iteration 6, loss = 2.58067556\n",
      "Iteration 7, loss = 2.58034068\n",
      "Iteration 8, loss = 2.58029290\n",
      "Iteration 9, loss = 2.58006526\n",
      "Iteration 10, loss = 2.57963138\n",
      "Iteration 11, loss = 2.57929763\n",
      "Iteration 12, loss = 2.57909031\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.57001588\n",
      "Iteration 2, loss = 2.62077745\n",
      "Iteration 3, loss = 2.62059149\n",
      "Iteration 4, loss = 2.62024251\n",
      "Iteration 5, loss = 2.61991902\n",
      "Iteration 6, loss = 2.61952953\n",
      "Iteration 7, loss = 2.61920286\n",
      "Iteration 8, loss = 2.61902641\n",
      "Iteration 9, loss = 2.61870669\n",
      "Iteration 10, loss = 2.61832593\n",
      "Iteration 11, loss = 2.61829332\n",
      "Iteration 12, loss = 2.61778072\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.17621482\n",
      "Iteration 2, loss = 2.18789345\n",
      "Iteration 3, loss = 2.18772223\n",
      "Iteration 4, loss = 2.18736516\n",
      "Iteration 5, loss = 2.18720788\n",
      "Iteration 6, loss = 2.18702269\n",
      "Iteration 7, loss = 2.18673058\n",
      "Iteration 8, loss = 2.18651039\n",
      "Iteration 9, loss = 2.18628690\n",
      "Iteration 10, loss = 2.18593317\n",
      "Iteration 11, loss = 2.18594062\n",
      "Iteration 12, loss = 2.18557086\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83906928\n",
      "Iteration 2, loss = 1.83996624\n",
      "Iteration 3, loss = 1.83988026\n",
      "Iteration 4, loss = 1.83971266\n",
      "Iteration 5, loss = 1.83934212\n",
      "Iteration 6, loss = 1.83940484\n",
      "Iteration 7, loss = 1.83907073\n",
      "Iteration 8, loss = 1.83890205\n",
      "Iteration 9, loss = 1.83883700\n",
      "Iteration 10, loss = 1.83859954\n",
      "Iteration 11, loss = 1.83845246\n",
      "Iteration 12, loss = 1.83822985\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68022700\n",
      "Iteration 2, loss = 0.66046384\n",
      "Iteration 3, loss = 0.66043211\n",
      "Iteration 4, loss = 0.66037872\n",
      "Iteration 5, loss = 0.66043884\n",
      "Iteration 6, loss = 0.66037214\n",
      "Iteration 7, loss = 0.66034000\n",
      "Iteration 8, loss = 0.66035662\n",
      "Iteration 9, loss = 0.66031782\n",
      "Iteration 10, loss = 0.66031286\n",
      "Iteration 11, loss = 0.66031180\n",
      "Iteration 12, loss = 0.66027914\n",
      "Iteration 13, loss = 0.66027930\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76016008\n",
      "Iteration 2, loss = 0.66070827\n",
      "Iteration 3, loss = 0.66075187\n",
      "Iteration 4, loss = 0.66058978\n",
      "Iteration 5, loss = 0.66092680\n",
      "Iteration 6, loss = 0.66084389\n",
      "Iteration 7, loss = 0.66063495\n",
      "Iteration 8, loss = 0.66087097\n",
      "Iteration 9, loss = 0.66063782\n",
      "Iteration 10, loss = 0.66069290\n",
      "Iteration 11, loss = 0.66062534\n",
      "Iteration 12, loss = 0.66072376\n",
      "Iteration 13, loss = 0.66058111\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75543520\n",
      "Iteration 2, loss = 0.66118158\n",
      "Iteration 3, loss = 0.66124577\n",
      "Iteration 4, loss = 0.66096444\n",
      "Iteration 5, loss = 0.66131105\n",
      "Iteration 6, loss = 0.66111461\n",
      "Iteration 7, loss = 0.66109953\n",
      "Iteration 8, loss = 0.66121960\n",
      "Iteration 9, loss = 0.66129308\n",
      "Iteration 10, loss = 0.66120434\n",
      "Iteration 11, loss = 0.66109225\n",
      "Iteration 12, loss = 0.66114431\n",
      "Iteration 13, loss = 0.66113973\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75373803\n",
      "Iteration 2, loss = 0.66245777\n",
      "Iteration 3, loss = 0.66242641\n",
      "Iteration 4, loss = 0.66246775\n",
      "Iteration 5, loss = 0.66249422\n",
      "Iteration 6, loss = 0.66264015\n",
      "Iteration 7, loss = 0.66255623\n",
      "Iteration 8, loss = 0.66237953\n",
      "Iteration 9, loss = 0.66248828\n",
      "Iteration 10, loss = 0.66243903\n",
      "Iteration 11, loss = 0.66233801\n",
      "Iteration 12, loss = 0.66263358\n",
      "Iteration 13, loss = 0.66233128\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74808579\n",
      "Iteration 2, loss = 0.66058614\n",
      "Iteration 3, loss = 0.66051682\n",
      "Iteration 4, loss = 0.66047890\n",
      "Iteration 5, loss = 0.66052173\n",
      "Iteration 6, loss = 0.66055435\n",
      "Iteration 7, loss = 0.66050601\n",
      "Iteration 8, loss = 0.66043086\n",
      "Iteration 9, loss = 0.66054994\n",
      "Iteration 10, loss = 0.66047611\n",
      "Iteration 11, loss = 0.66035318\n",
      "Iteration 12, loss = 0.66054967\n",
      "Iteration 13, loss = 0.66040311\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74577138\n",
      "Iteration 2, loss = 0.65528135\n",
      "Iteration 3, loss = 0.65529707\n",
      "Iteration 4, loss = 0.65529961\n",
      "Iteration 5, loss = 0.65527190\n",
      "Iteration 6, loss = 0.65538102\n",
      "Iteration 7, loss = 0.65531620\n",
      "Iteration 8, loss = 0.65513656\n",
      "Iteration 9, loss = 0.65513095\n",
      "Iteration 10, loss = 0.65515318\n",
      "Iteration 11, loss = 0.65531877\n",
      "Iteration 12, loss = 0.65527835\n",
      "Iteration 13, loss = 0.65511651\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.93751007\n",
      "Iteration 2, loss = 0.91551899\n",
      "Iteration 3, loss = 0.91527946\n",
      "Iteration 4, loss = 0.91502136\n",
      "Iteration 5, loss = 0.91485234\n",
      "Iteration 6, loss = 0.91464763\n",
      "Iteration 7, loss = 0.91449877\n",
      "Iteration 8, loss = 0.91422765\n",
      "Iteration 9, loss = 0.91400584\n",
      "Iteration 10, loss = 0.91383960\n",
      "Iteration 11, loss = 0.91362746\n",
      "Iteration 12, loss = 0.91343676\n",
      "Iteration 13, loss = 0.91327226\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44985046\n",
      "Iteration 2, loss = 1.38733849\n",
      "Iteration 3, loss = 1.38727686\n",
      "Iteration 4, loss = 1.38732742\n",
      "Iteration 5, loss = 1.38700582\n",
      "Iteration 6, loss = 1.38688919\n",
      "Iteration 7, loss = 1.38692950\n",
      "Iteration 8, loss = 1.38688811\n",
      "Iteration 9, loss = 1.38671618\n",
      "Iteration 10, loss = 1.38647130\n",
      "Iteration 11, loss = 1.38624766\n",
      "Iteration 12, loss = 1.38624439\n",
      "Iteration 13, loss = 1.38608165\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 139.23292986\n",
      "Iteration 2, loss = 277.83343150\n",
      "Iteration 3, loss = 277.79584328\n",
      "Iteration 4, loss = 277.75538543\n",
      "Iteration 5, loss = 277.71417455\n",
      "Iteration 6, loss = 277.67337823\n",
      "Iteration 7, loss = 277.63260849\n",
      "Iteration 8, loss = 277.59170136\n",
      "Iteration 9, loss = 277.55069011\n",
      "Iteration 10, loss = 277.51007535\n",
      "Iteration 11, loss = 277.46887357\n",
      "Iteration 12, loss = 277.42805524\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 13.68391088\n",
      "Iteration 2, loss = 71.66797330\n",
      "Iteration 3, loss = 71.79866715\n",
      "Iteration 4, loss = 71.78820752\n",
      "Iteration 5, loss = 71.77786378\n",
      "Iteration 6, loss = 71.76732513\n",
      "Iteration 7, loss = 71.75661474\n",
      "Iteration 8, loss = 71.74618058\n",
      "Iteration 9, loss = 71.73587108\n",
      "Iteration 10, loss = 71.72526982\n",
      "Iteration 11, loss = 71.71467607\n",
      "Iteration 12, loss = 71.70436855\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.20831956\n",
      "Iteration 2, loss = 1.11789738\n",
      "Iteration 3, loss = 1.11782881\n",
      "Iteration 4, loss = 1.11782284\n",
      "Iteration 5, loss = 1.11780241\n",
      "Iteration 6, loss = 1.11750172\n",
      "Iteration 7, loss = 1.11742802\n",
      "Iteration 8, loss = 1.11735473\n",
      "Iteration 9, loss = 1.11726155\n",
      "Iteration 10, loss = 1.11722537\n",
      "Iteration 11, loss = 1.11710261\n",
      "Iteration 12, loss = 1.11715809\n",
      "Iteration 13, loss = 1.11694765\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.17113677\n",
      "Iteration 2, loss = 1.08328532\n",
      "Iteration 3, loss = 1.08309729\n",
      "Iteration 4, loss = 1.08314118\n",
      "Iteration 5, loss = 1.08284604\n",
      "Iteration 6, loss = 1.08302731\n",
      "Iteration 7, loss = 1.08276530\n",
      "Iteration 8, loss = 1.08255180\n",
      "Iteration 9, loss = 1.08269528\n",
      "Iteration 10, loss = 1.08257452\n",
      "Iteration 11, loss = 1.08247188\n",
      "Iteration 12, loss = 1.08249492\n",
      "Iteration 13, loss = 1.08268491\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65467402\n",
      "Iteration 2, loss = 0.64410137\n",
      "Iteration 3, loss = 0.64405549\n",
      "Iteration 4, loss = 0.64397940\n",
      "Iteration 5, loss = 0.64404538\n",
      "Iteration 6, loss = 0.64406372\n",
      "Iteration 7, loss = 0.64399286\n",
      "Iteration 8, loss = 0.64409582\n",
      "Iteration 9, loss = 0.64396638\n",
      "Iteration 10, loss = 0.64399444\n",
      "Iteration 11, loss = 0.64402746\n",
      "Iteration 12, loss = 0.64397566\n",
      "Iteration 13, loss = 0.64402768\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71369768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.64573935\n",
      "Iteration 3, loss = 0.64589351\n",
      "Iteration 4, loss = 0.64576538\n",
      "Iteration 5, loss = 0.64580237\n",
      "Iteration 6, loss = 0.64565078\n",
      "Iteration 7, loss = 0.64603230\n",
      "Iteration 8, loss = 0.64522163\n",
      "Iteration 9, loss = 0.64571935\n",
      "Iteration 10, loss = 0.64561643\n",
      "Iteration 11, loss = 0.64564052\n",
      "Iteration 12, loss = 0.64555169\n",
      "Iteration 13, loss = 0.64556172\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70794272\n",
      "Iteration 2, loss = 0.64619114\n",
      "Iteration 3, loss = 0.64589483\n",
      "Iteration 4, loss = 0.64595652\n",
      "Iteration 5, loss = 0.64591146\n",
      "Iteration 6, loss = 0.64567543\n",
      "Iteration 7, loss = 0.64597947\n",
      "Iteration 8, loss = 0.64568677\n",
      "Iteration 9, loss = 0.64578218\n",
      "Iteration 10, loss = 0.64549781\n",
      "Iteration 11, loss = 0.64569571\n",
      "Iteration 12, loss = 0.64558953\n",
      "Iteration 13, loss = 0.64557341\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69602664\n",
      "Iteration 2, loss = 0.64465206\n",
      "Iteration 3, loss = 0.64431587\n",
      "Iteration 4, loss = 0.64430576\n",
      "Iteration 5, loss = 0.64418293\n",
      "Iteration 6, loss = 0.64426941\n",
      "Iteration 7, loss = 0.64413437\n",
      "Iteration 8, loss = 0.64436601\n",
      "Iteration 9, loss = 0.64429241\n",
      "Iteration 10, loss = 0.64441336\n",
      "Iteration 11, loss = 0.64443854\n",
      "Iteration 12, loss = 0.64406906\n",
      "Iteration 13, loss = 0.64404942\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69938001\n",
      "Iteration 2, loss = 0.64514487\n",
      "Iteration 3, loss = 0.64502220\n",
      "Iteration 4, loss = 0.64473620\n",
      "Iteration 5, loss = 0.64466940\n",
      "Iteration 6, loss = 0.64456576\n",
      "Iteration 7, loss = 0.64456447\n",
      "Iteration 8, loss = 0.64470062\n",
      "Iteration 9, loss = 0.64455824\n",
      "Iteration 10, loss = 0.64469500\n",
      "Iteration 11, loss = 0.64449826\n",
      "Iteration 12, loss = 0.64455971\n",
      "Iteration 13, loss = 0.64446923\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70294702\n",
      "Iteration 2, loss = 0.64570171\n",
      "Iteration 3, loss = 0.64546837\n",
      "Iteration 4, loss = 0.64501020\n",
      "Iteration 5, loss = 0.64515935\n",
      "Iteration 6, loss = 0.64505895\n",
      "Iteration 7, loss = 0.64500325\n",
      "Iteration 8, loss = 0.64513858\n",
      "Iteration 9, loss = 0.64496749\n",
      "Iteration 10, loss = 0.64509508\n",
      "Iteration 11, loss = 0.64495319\n",
      "Iteration 12, loss = 0.64503287\n",
      "Iteration 13, loss = 0.64499321\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.31746117\n",
      "Iteration 2, loss = 15.58043119\n",
      "Iteration 3, loss = 15.56941602\n",
      "Iteration 4, loss = 15.55846110\n",
      "Iteration 5, loss = 15.54752997\n",
      "Iteration 6, loss = 15.53655024\n",
      "Iteration 7, loss = 15.52560573\n",
      "Iteration 8, loss = 15.51469414\n",
      "Iteration 9, loss = 15.50374845\n",
      "Iteration 10, loss = 15.49272393\n",
      "Iteration 11, loss = 15.48193965\n",
      "Iteration 12, loss = 15.47105389\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20.83367267\n",
      "Iteration 2, loss = 22.93570858\n",
      "Iteration 3, loss = 22.93246402\n",
      "Iteration 4, loss = 22.92908394\n",
      "Iteration 5, loss = 22.92599316\n",
      "Iteration 6, loss = 22.92246537\n",
      "Iteration 7, loss = 22.91914949\n",
      "Iteration 8, loss = 22.91592863\n",
      "Iteration 9, loss = 22.91273896\n",
      "Iteration 10, loss = 22.90945154\n",
      "Iteration 11, loss = 22.90607035\n",
      "Iteration 12, loss = 22.90273738\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 34.31426032\n",
      "Iteration 2, loss = 37.90880373\n",
      "Iteration 3, loss = 37.90339875\n",
      "Iteration 4, loss = 37.89780778\n",
      "Iteration 5, loss = 37.89243363\n",
      "Iteration 6, loss = 37.88688404\n",
      "Iteration 7, loss = 37.88142061\n",
      "Iteration 8, loss = 37.87582305\n",
      "Iteration 9, loss = 37.87049267\n",
      "Iteration 10, loss = 37.86505533\n",
      "Iteration 11, loss = 37.85939740\n",
      "Iteration 12, loss = 37.85385105\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 33.83627552\n",
      "Iteration 2, loss = 37.37300897\n",
      "Iteration 3, loss = 37.36765055\n",
      "Iteration 4, loss = 37.36232082\n",
      "Iteration 5, loss = 37.35697816\n",
      "Iteration 6, loss = 37.35145876\n",
      "Iteration 7, loss = 37.34597974\n",
      "Iteration 8, loss = 37.34064223\n",
      "Iteration 9, loss = 37.33520938\n",
      "Iteration 10, loss = 37.32985442\n",
      "Iteration 11, loss = 37.32429310\n",
      "Iteration 12, loss = 37.31894003\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 35.14209671\n",
      "Iteration 2, loss = 38.82123773\n",
      "Iteration 3, loss = 38.81572530\n",
      "Iteration 4, loss = 38.81017760\n",
      "Iteration 5, loss = 38.80448611\n",
      "Iteration 6, loss = 38.79880430\n",
      "Iteration 7, loss = 38.79324875\n",
      "Iteration 8, loss = 38.78765721\n",
      "Iteration 9, loss = 38.78196464\n",
      "Iteration 10, loss = 38.77636712\n",
      "Iteration 11, loss = 38.77059645\n",
      "Iteration 12, loss = 38.76523651\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 44.42641070\n",
      "Iteration 2, loss = 49.13832721\n",
      "Iteration 3, loss = 49.13113898\n",
      "Iteration 4, loss = 49.12395791\n",
      "Iteration 5, loss = 49.11693035\n",
      "Iteration 6, loss = 49.10994218\n",
      "Iteration 7, loss = 49.10258072\n",
      "Iteration 8, loss = 49.09550656\n",
      "Iteration 9, loss = 49.08828424\n",
      "Iteration 10, loss = 49.08119604\n",
      "Iteration 11, loss = 49.07402949\n",
      "Iteration 12, loss = 49.06692660\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7224.75862448\n",
      "Iteration 2, loss = 7367.55369211\n",
      "Iteration 3, loss = 7362.14136545\n",
      "Iteration 4, loss = 7356.73303549\n",
      "Iteration 5, loss = 7351.32870614\n",
      "Iteration 6, loss = 7345.92830408\n",
      "Iteration 7, loss = 7340.53193426\n",
      "Iteration 8, loss = 7335.13956089\n",
      "Iteration 9, loss = 7329.75104694\n",
      "Iteration 10, loss = 7324.36650136\n",
      "Iteration 11, loss = 7318.98599961\n",
      "Iteration 12, loss = 7313.60942838\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8620.88170635\n",
      "Iteration 2, loss = 9580.60127896\n",
      "Iteration 3, loss = 9579.18921402\n",
      "Iteration 4, loss = 9577.77715729\n",
      "Iteration 5, loss = 9576.36535553\n",
      "Iteration 6, loss = 9574.95380325\n",
      "Iteration 7, loss = 9573.54220300\n",
      "Iteration 8, loss = 9572.13101216\n",
      "Iteration 9, loss = 9570.72008541\n",
      "Iteration 10, loss = 9569.30909994\n",
      "Iteration 11, loss = 9567.89872468\n",
      "Iteration 12, loss = 9566.48829971\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7272.20106008\n",
      "Iteration 2, loss = 8081.73319823\n",
      "Iteration 3, loss = 8080.54213450\n",
      "Iteration 4, loss = 8079.35090800\n",
      "Iteration 5, loss = 8078.16002242\n",
      "Iteration 6, loss = 8076.96927265\n",
      "Iteration 7, loss = 8075.77858742\n",
      "Iteration 8, loss = 8074.58822793\n",
      "Iteration 9, loss = 8073.39815139\n",
      "Iteration 10, loss = 8072.20783942\n",
      "Iteration 11, loss = 8071.01802802\n",
      "Iteration 12, loss = 8069.82829715\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8165.91226980\n",
      "Iteration 2, loss = 9074.97038251\n",
      "Iteration 3, loss = 9073.63343529\n",
      "Iteration 4, loss = 9072.29566822\n",
      "Iteration 5, loss = 9070.95834511\n",
      "Iteration 6, loss = 9069.62126002\n",
      "Iteration 7, loss = 9068.28445816\n",
      "Iteration 8, loss = 9066.94802533\n",
      "Iteration 9, loss = 9065.61150916\n",
      "Iteration 10, loss = 9064.27509302\n",
      "Iteration 11, loss = 9062.93907640\n",
      "Iteration 12, loss = 9061.60346126\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6612.05193117\n",
      "Iteration 2, loss = 7348.08900297\n",
      "Iteration 3, loss = 7347.00614704\n",
      "Iteration 4, loss = 7345.92291901\n",
      "Iteration 5, loss = 7344.84020528\n",
      "Iteration 6, loss = 7343.75753341\n",
      "Iteration 7, loss = 7342.67497274\n",
      "Iteration 8, loss = 7341.59295963\n",
      "Iteration 9, loss = 7340.51083540\n",
      "Iteration 10, loss = 7339.42878732\n",
      "Iteration 11, loss = 7338.34690519\n",
      "Iteration 12, loss = 7337.26551108\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17540.91414526\n",
      "Iteration 2, loss = 19514.59203425\n",
      "Iteration 3, loss = 19511.71651426\n",
      "Iteration 4, loss = 19508.84029680\n",
      "Iteration 5, loss = 19505.96492725\n",
      "Iteration 6, loss = 19503.08971104\n",
      "Iteration 7, loss = 19500.21536127\n",
      "Iteration 8, loss = 19497.34100217\n",
      "Iteration 9, loss = 19494.46736076\n",
      "Iteration 10, loss = 19491.59401523\n",
      "Iteration 11, loss = 19488.72092512\n",
      "Iteration 12, loss = 19485.84850751\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2611.86736614\n",
      "Iteration 2, loss = 2663.40233652\n",
      "Iteration 3, loss = 2661.44600631\n",
      "Iteration 4, loss = 2659.49120648\n",
      "Iteration 5, loss = 2657.53778924\n",
      "Iteration 6, loss = 2655.58586980\n",
      "Iteration 7, loss = 2653.63531724\n",
      "Iteration 8, loss = 2651.68621885\n",
      "Iteration 9, loss = 2649.73854666\n",
      "Iteration 10, loss = 2647.79240516\n",
      "Iteration 11, loss = 2645.84751009\n",
      "Iteration 12, loss = 2643.90413577\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2661.14997262\n",
      "Iteration 2, loss = 2957.05147834\n",
      "Iteration 3, loss = 2956.61571531\n",
      "Iteration 4, loss = 2956.17987481\n",
      "Iteration 5, loss = 2955.74432885\n",
      "Iteration 6, loss = 2955.30841649\n",
      "Iteration 7, loss = 2954.87326100\n",
      "Iteration 8, loss = 2954.43754339\n",
      "Iteration 9, loss = 2954.00217085\n",
      "Iteration 10, loss = 2953.56658799\n",
      "Iteration 11, loss = 2953.13136529\n",
      "Iteration 12, loss = 2952.69609159\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2979.73613128\n",
      "Iteration 2, loss = 3311.11665025\n",
      "Iteration 3, loss = 3310.62853473\n",
      "Iteration 4, loss = 3310.14071739\n",
      "Iteration 5, loss = 3309.65291360\n",
      "Iteration 6, loss = 3309.16467082\n",
      "Iteration 7, loss = 3308.67741085\n",
      "Iteration 8, loss = 3308.18953612\n",
      "Iteration 9, loss = 3307.70217518\n",
      "Iteration 10, loss = 3307.21435129\n",
      "Iteration 11, loss = 3306.72687131\n",
      "Iteration 12, loss = 3306.23960279\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2309.92743257\n",
      "Iteration 2, loss = 2566.67154911\n",
      "Iteration 3, loss = 2566.29346391\n",
      "Iteration 4, loss = 2565.91518379\n",
      "Iteration 5, loss = 2565.53709559\n",
      "Iteration 6, loss = 2565.15900903\n",
      "Iteration 7, loss = 2564.78112610\n",
      "Iteration 8, loss = 2564.40305560\n",
      "Iteration 9, loss = 2564.02521040\n",
      "Iteration 10, loss = 2563.64721371\n",
      "Iteration 11, loss = 2563.26940296\n",
      "Iteration 12, loss = 2562.89172714\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2430.97250454\n",
      "Iteration 2, loss = 2701.20094266\n",
      "Iteration 3, loss = 2700.80299816\n",
      "Iteration 4, loss = 2700.40500674\n",
      "Iteration 5, loss = 2700.00704758\n",
      "Iteration 6, loss = 2699.60897825\n",
      "Iteration 7, loss = 2699.21141237\n",
      "Iteration 8, loss = 2698.81346902\n",
      "Iteration 9, loss = 2698.41572702\n",
      "Iteration 10, loss = 2698.01807008\n",
      "Iteration 11, loss = 2697.62039179\n",
      "Iteration 12, loss = 2697.22291199\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2822.35913409\n",
      "Iteration 2, loss = 3136.20121561\n",
      "Iteration 3, loss = 3135.73910226\n",
      "Iteration 4, loss = 3135.27693849\n",
      "Iteration 5, loss = 3134.81545866\n",
      "Iteration 6, loss = 3134.35303214\n",
      "Iteration 7, loss = 3133.89106058\n",
      "Iteration 8, loss = 3133.42925002\n",
      "Iteration 9, loss = 3132.96753786\n",
      "Iteration 10, loss = 3132.50580285\n",
      "Iteration 11, loss = 3132.04408085\n",
      "Iteration 12, loss = 3131.58241698\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 552.03399552\n",
      "Iteration 2, loss = 562.89990461\n",
      "Iteration 3, loss = 562.48675581\n",
      "Iteration 4, loss = 562.07397710\n",
      "Iteration 5, loss = 561.66149823\n",
      "Iteration 6, loss = 561.24934908\n",
      "Iteration 7, loss = 560.83752584\n",
      "Iteration 8, loss = 560.42594572\n",
      "Iteration 9, loss = 560.01464998\n",
      "Iteration 10, loss = 559.60364806\n",
      "Iteration 11, loss = 559.19309647\n",
      "Iteration 12, loss = 558.78271884\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 901.88343014\n",
      "Iteration 2, loss = 1002.09136540\n",
      "Iteration 3, loss = 1001.94336732\n",
      "Iteration 4, loss = 1001.79587932\n",
      "Iteration 5, loss = 1001.64822216\n",
      "Iteration 6, loss = 1001.50061216\n",
      "Iteration 7, loss = 1001.35302422\n",
      "Iteration 8, loss = 1001.20549975\n",
      "Iteration 9, loss = 1001.05805966\n",
      "Iteration 10, loss = 1000.91046496\n",
      "Iteration 11, loss = 1000.76292122\n",
      "Iteration 12, loss = 1000.61558097\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 727.09543850\n",
      "Iteration 2, loss = 807.85431964\n",
      "Iteration 3, loss = 807.73528291\n",
      "Iteration 4, loss = 807.61652674\n",
      "Iteration 5, loss = 807.49741241\n",
      "Iteration 6, loss = 807.37841384\n",
      "Iteration 7, loss = 807.25957816\n",
      "Iteration 8, loss = 807.14059197\n",
      "Iteration 9, loss = 807.02171616\n",
      "Iteration 10, loss = 806.90281980\n",
      "Iteration 11, loss = 806.78394942\n",
      "Iteration 12, loss = 806.66509184\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 238.93070375\n",
      "Iteration 2, loss = 265.31632931\n",
      "Iteration 3, loss = 265.27721835\n",
      "Iteration 4, loss = 265.23819923\n",
      "Iteration 5, loss = 265.19925565\n",
      "Iteration 6, loss = 265.16010103\n",
      "Iteration 7, loss = 265.12137930\n",
      "Iteration 8, loss = 265.08227206\n",
      "Iteration 9, loss = 265.04321654\n",
      "Iteration 10, loss = 265.00420995\n",
      "Iteration 11, loss = 264.96508014\n",
      "Iteration 12, loss = 264.92635206\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 462.19497205\n",
      "Iteration 2, loss = 513.43637991\n",
      "Iteration 3, loss = 513.36088502\n",
      "Iteration 4, loss = 513.28510206\n",
      "Iteration 5, loss = 513.20944200\n",
      "Iteration 6, loss = 513.13390961\n",
      "Iteration 7, loss = 513.05845501\n",
      "Iteration 8, loss = 512.98294643\n",
      "Iteration 9, loss = 512.90733790\n",
      "Iteration 10, loss = 512.83180289\n",
      "Iteration 11, loss = 512.75623205\n",
      "Iteration 12, loss = 512.68092688\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 431.37172524\n",
      "Iteration 2, loss = 479.22873366\n",
      "Iteration 3, loss = 479.15799985\n",
      "Iteration 4, loss = 479.08739460\n",
      "Iteration 5, loss = 479.01708294\n",
      "Iteration 6, loss = 478.94658958\n",
      "Iteration 7, loss = 478.87601778\n",
      "Iteration 8, loss = 478.80546874\n",
      "Iteration 9, loss = 478.73508048\n",
      "Iteration 10, loss = 478.66457330\n",
      "Iteration 11, loss = 478.59428096\n",
      "Iteration 12, loss = 478.52353362\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7132.57926753\n",
      "Iteration 2, loss = 7802.16940941\n",
      "Iteration 3, loss = 7796.43776960\n",
      "Iteration 4, loss = 7790.71040978\n",
      "Iteration 5, loss = 7784.98726268\n",
      "Iteration 6, loss = 7779.26826801\n",
      "Iteration 7, loss = 7773.55349324\n",
      "Iteration 8, loss = 7767.84288589\n",
      "Iteration 9, loss = 7762.13656984\n",
      "Iteration 10, loss = 7756.43442469\n",
      "Iteration 11, loss = 7750.73643072\n",
      "Iteration 12, loss = 7745.04267933\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1518.77025518\n",
      "Iteration 2, loss = 1687.61242856\n",
      "Iteration 3, loss = 1687.36394642\n",
      "Iteration 4, loss = 1687.11514445\n",
      "Iteration 5, loss = 1686.86659082\n",
      "Iteration 6, loss = 1686.61787037\n",
      "Iteration 7, loss = 1686.36927591\n",
      "Iteration 8, loss = 1686.12098573\n",
      "Iteration 9, loss = 1685.87231950\n",
      "Iteration 10, loss = 1685.62399469\n",
      "Iteration 11, loss = 1685.37575242\n",
      "Iteration 12, loss = 1685.12710511\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1265.88923245\n",
      "Iteration 2, loss = 1406.57009027\n",
      "Iteration 3, loss = 1406.36292497\n",
      "Iteration 4, loss = 1406.15570900\n",
      "Iteration 5, loss = 1405.94873415\n",
      "Iteration 6, loss = 1405.74129296\n",
      "Iteration 7, loss = 1405.53415249\n",
      "Iteration 8, loss = 1405.32709189\n",
      "Iteration 9, loss = 1405.12000625\n",
      "Iteration 10, loss = 1404.91300982\n",
      "Iteration 11, loss = 1404.70624430\n",
      "Iteration 12, loss = 1404.49889983\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2216.29125883\n",
      "Iteration 2, loss = 2462.79791052\n",
      "Iteration 3, loss = 2462.43494330\n",
      "Iteration 4, loss = 2462.07200934\n",
      "Iteration 5, loss = 2461.70911584\n",
      "Iteration 6, loss = 2461.34632482\n",
      "Iteration 7, loss = 2460.98369156\n",
      "Iteration 8, loss = 2460.62117396\n",
      "Iteration 9, loss = 2460.25856049\n",
      "Iteration 10, loss = 2459.89572194\n",
      "Iteration 11, loss = 2459.53323383\n",
      "Iteration 12, loss = 2459.17069811\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1991.68259248\n",
      "Iteration 2, loss = 2213.17623157\n",
      "Iteration 3, loss = 2212.84995482\n",
      "Iteration 4, loss = 2212.52406028\n",
      "Iteration 5, loss = 2212.20107015\n",
      "Iteration 6, loss = 2211.87196039\n",
      "Iteration 7, loss = 2211.54598115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, loss = 2211.22018624\n",
      "Iteration 9, loss = 2210.89434654\n",
      "Iteration 10, loss = 2210.56845092\n",
      "Iteration 11, loss = 2210.24260346\n",
      "Iteration 12, loss = 2209.91693445\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1700.88736699\n",
      "Iteration 2, loss = 1889.99398733\n",
      "Iteration 3, loss = 1889.71546296\n",
      "Iteration 4, loss = 1889.43713495\n",
      "Iteration 5, loss = 1889.15871413\n",
      "Iteration 6, loss = 1888.88030289\n",
      "Iteration 7, loss = 1888.60196665\n",
      "Iteration 8, loss = 1888.32383706\n",
      "Iteration 9, loss = 1888.04551683\n",
      "Iteration 10, loss = 1887.76726759\n",
      "Iteration 11, loss = 1887.48925796\n",
      "Iteration 12, loss = 1887.21099877\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.43476217\n",
      "Iteration 2, loss = 18.75829789\n",
      "Iteration 3, loss = 18.74500528\n",
      "Iteration 4, loss = 18.73485766\n",
      "Iteration 5, loss = 18.72438019\n",
      "Iteration 6, loss = 18.71119631\n",
      "Iteration 7, loss = 18.69791269\n",
      "Iteration 8, loss = 18.68456624\n",
      "Iteration 9, loss = 18.67133035\n",
      "Iteration 10, loss = 18.65808613\n",
      "Iteration 11, loss = 18.64487947\n",
      "Iteration 12, loss = 18.63158189\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.03898995\n",
      "Iteration 2, loss = 16.47138615\n",
      "Iteration 3, loss = 16.46909134\n",
      "Iteration 4, loss = 16.46683818\n",
      "Iteration 5, loss = 16.46427029\n",
      "Iteration 6, loss = 16.46213172\n",
      "Iteration 7, loss = 16.45966306\n",
      "Iteration 8, loss = 16.45751081\n",
      "Iteration 9, loss = 16.45516792\n",
      "Iteration 10, loss = 16.45272128\n",
      "Iteration 11, loss = 16.45042600\n",
      "Iteration 12, loss = 16.44811996\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.71225677\n",
      "Iteration 2, loss = 19.44161879\n",
      "Iteration 3, loss = 19.43881036\n",
      "Iteration 4, loss = 19.43616445\n",
      "Iteration 5, loss = 19.43320743\n",
      "Iteration 6, loss = 19.43069906\n",
      "Iteration 7, loss = 19.42784185\n",
      "Iteration 8, loss = 19.42507866\n",
      "Iteration 9, loss = 19.42237442\n",
      "Iteration 10, loss = 19.41960348\n",
      "Iteration 11, loss = 19.41671890\n",
      "Iteration 12, loss = 19.41391599\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.27475224\n",
      "Iteration 2, loss = 16.74324011\n",
      "Iteration 3, loss = 16.74099352\n",
      "Iteration 4, loss = 16.73866715\n",
      "Iteration 5, loss = 16.77047899\n",
      "Iteration 6, loss = 16.73925722\n",
      "Iteration 7, loss = 16.73676317\n",
      "Iteration 8, loss = 16.73451322\n",
      "Iteration 9, loss = 16.73204982\n",
      "Iteration 10, loss = 16.72972060\n",
      "Iteration 11, loss = 16.72756686\n",
      "Iteration 12, loss = 16.72492700\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15.92177500\n",
      "Iteration 2, loss = 17.45983755\n",
      "Iteration 3, loss = 17.45736157\n",
      "Iteration 4, loss = 17.45482435\n",
      "Iteration 5, loss = 17.45237532\n",
      "Iteration 6, loss = 17.44993112\n",
      "Iteration 7, loss = 17.44736662\n",
      "Iteration 8, loss = 17.44509997\n",
      "Iteration 9, loss = 17.44250401\n",
      "Iteration 10, loss = 17.44011968\n",
      "Iteration 11, loss = 17.43765455\n",
      "Iteration 12, loss = 17.43500596\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.01940515\n",
      "Iteration 2, loss = 19.79029505\n",
      "Iteration 3, loss = 19.78764806\n",
      "Iteration 4, loss = 19.78473380\n",
      "Iteration 5, loss = 19.84245918\n",
      "Iteration 6, loss = 19.78534555\n",
      "Iteration 7, loss = 19.78204017\n",
      "Iteration 8, loss = 19.77927290\n",
      "Iteration 9, loss = 19.77637657\n",
      "Iteration 10, loss = 19.77349057\n",
      "Iteration 11, loss = 19.77047811\n",
      "Iteration 12, loss = 19.76778356\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 15601.71809984\n",
      "Iteration 2, loss = 15910.32336306\n",
      "Iteration 3, loss = 15898.63499717\n",
      "Iteration 4, loss = 15886.95514518\n",
      "Iteration 5, loss = 15875.28382695\n",
      "Iteration 6, loss = 15863.62117845\n",
      "Iteration 7, loss = 15851.96701545\n",
      "Iteration 8, loss = 15840.32153943\n",
      "Iteration 9, loss = 15828.68444768\n",
      "Iteration 10, loss = 15817.05604831\n",
      "Iteration 11, loss = 15805.43610875\n",
      "Iteration 12, loss = 15793.82470833\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18043.91244101\n",
      "Iteration 2, loss = 20052.88867256\n",
      "Iteration 3, loss = 20049.93283518\n",
      "Iteration 4, loss = 20046.97717520\n",
      "Iteration 5, loss = 20044.02202989\n",
      "Iteration 6, loss = 20041.06723322\n",
      "Iteration 7, loss = 20038.11291357\n",
      "Iteration 8, loss = 20035.15905358\n",
      "Iteration 9, loss = 20032.20555344\n",
      "Iteration 10, loss = 20029.25250396\n",
      "Iteration 11, loss = 20026.29992176\n",
      "Iteration 12, loss = 20023.34789057\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 28840.51394295\n",
      "Iteration 2, loss = 32051.71558077\n",
      "Iteration 3, loss = 32046.98456424\n",
      "Iteration 4, loss = 32042.25656310\n",
      "Iteration 5, loss = 32037.53300050\n",
      "Iteration 6, loss = 32032.81028707\n",
      "Iteration 7, loss = 32028.08805163\n",
      "Iteration 8, loss = 32023.36659867\n",
      "Iteration 9, loss = 32018.64574987\n",
      "Iteration 10, loss = 32013.92574566\n",
      "Iteration 11, loss = 32009.20637434\n",
      "Iteration 12, loss = 32004.48779583\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 20466.98064108\n",
      "Iteration 2, loss = 22745.77387262\n",
      "Iteration 3, loss = 22742.42005224\n",
      "Iteration 4, loss = 22739.06783309\n",
      "Iteration 5, loss = 22735.71598131\n",
      "Iteration 6, loss = 22732.36486686\n",
      "Iteration 7, loss = 22729.01385930\n",
      "Iteration 8, loss = 22725.66350516\n",
      "Iteration 9, loss = 22722.31365311\n",
      "Iteration 10, loss = 22718.96438580\n",
      "Iteration 11, loss = 22715.61551615\n",
      "Iteration 12, loss = 22712.26704628\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 22492.83122943\n",
      "Iteration 2, loss = 24997.20971099\n",
      "Iteration 3, loss = 24993.52510912\n",
      "Iteration 4, loss = 24989.84174228\n",
      "Iteration 5, loss = 24986.15524954\n",
      "Iteration 6, loss = 24982.47236497\n",
      "Iteration 7, loss = 24978.78930282\n",
      "Iteration 8, loss = 24975.10669964\n",
      "Iteration 9, loss = 24971.42518368\n",
      "Iteration 10, loss = 24967.74462796\n",
      "Iteration 11, loss = 24964.06355276\n",
      "Iteration 12, loss = 24960.38398703\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17566.16363616\n",
      "Iteration 2, loss = 19522.96095839\n",
      "Iteration 3, loss = 19520.08375507\n",
      "Iteration 4, loss = 19517.20636740\n",
      "Iteration 5, loss = 19514.32966994\n",
      "Iteration 6, loss = 19511.45334106\n",
      "Iteration 7, loss = 19508.57738567\n",
      "Iteration 8, loss = 19505.70214904\n",
      "Iteration 9, loss = 19502.82703282\n",
      "Iteration 10, loss = 19499.95265843\n",
      "Iteration 11, loss = 19497.07841930\n",
      "Iteration 12, loss = 19494.20457684\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 33084.78339255\n",
      "Iteration 2, loss = 33921.71577626\n",
      "Iteration 3, loss = 33896.79473145\n",
      "Iteration 4, loss = 33871.89214994\n",
      "Iteration 5, loss = 33847.00772395\n",
      "Iteration 6, loss = 33822.14179962\n",
      "Iteration 7, loss = 33797.29386323\n",
      "Iteration 8, loss = 33772.46431622\n",
      "Iteration 9, loss = 33747.65302976\n",
      "Iteration 10, loss = 33722.85997411\n",
      "Iteration 11, loss = 33698.08507021\n",
      "Iteration 12, loss = 33673.32838461\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 414.93882918\n",
      "Iteration 2, loss = 460.73055870\n",
      "Iteration 3, loss = 460.66278162\n",
      "Iteration 4, loss = 460.59482640\n",
      "Iteration 5, loss = 460.52713468\n",
      "Iteration 6, loss = 460.45918391\n",
      "Iteration 7, loss = 460.39158186\n",
      "Iteration 8, loss = 460.32385839\n",
      "Iteration 9, loss = 460.25599157\n",
      "Iteration 10, loss = 460.18824651\n",
      "Iteration 11, loss = 460.12044742\n",
      "Iteration 12, loss = 460.05288324\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 612.64584123\n",
      "Iteration 2, loss = 680.45500445\n",
      "Iteration 3, loss = 680.35485188\n",
      "Iteration 4, loss = 680.25461783\n",
      "Iteration 5, loss = 680.15454565\n",
      "Iteration 6, loss = 680.05422772\n",
      "Iteration 7, loss = 679.95403287\n",
      "Iteration 8, loss = 679.85400951\n",
      "Iteration 9, loss = 679.75380138\n",
      "Iteration 10, loss = 679.65373039\n",
      "Iteration 11, loss = 679.55367309\n",
      "Iteration 12, loss = 679.45371236\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 448.95139270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 498.52208578\n",
      "Iteration 3, loss = 498.44851072\n",
      "Iteration 4, loss = 498.37517736\n",
      "Iteration 5, loss = 498.30170330\n",
      "Iteration 6, loss = 498.22854194\n",
      "Iteration 7, loss = 498.15508677\n",
      "Iteration 8, loss = 498.08168341\n",
      "Iteration 9, loss = 498.00847747\n",
      "Iteration 10, loss = 497.93512518\n",
      "Iteration 11, loss = 497.86187787\n",
      "Iteration 12, loss = 497.78848428\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 522.08570898\n",
      "Iteration 2, loss = 579.79896481\n",
      "Iteration 3, loss = 579.71364092\n",
      "Iteration 4, loss = 579.62822080\n",
      "Iteration 5, loss = 579.54273048\n",
      "Iteration 6, loss = 579.45744719\n",
      "Iteration 7, loss = 579.37213264\n",
      "Iteration 8, loss = 579.28682936\n",
      "Iteration 9, loss = 579.20150027\n",
      "Iteration 10, loss = 579.11642204\n",
      "Iteration 11, loss = 579.03105505\n",
      "Iteration 12, loss = 578.94564411\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 431.67833850\n",
      "Iteration 2, loss = 479.39315470\n",
      "Iteration 3, loss = 479.32261734\n",
      "Iteration 4, loss = 479.25194017\n",
      "Iteration 5, loss = 479.18145596\n",
      "Iteration 6, loss = 479.11096848\n",
      "Iteration 7, loss = 479.04045209\n",
      "Iteration 8, loss = 478.96980241\n",
      "Iteration 9, loss = 478.89936296\n",
      "Iteration 10, loss = 478.82893363\n",
      "Iteration 11, loss = 478.75845868\n",
      "Iteration 12, loss = 478.68793824\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5468172.93143831\n",
      "Iteration 2, loss = 5591778.98306923\n",
      "Iteration 3, loss = 5587670.84381635\n",
      "Iteration 4, loss = 5583565.72271410\n",
      "Iteration 5, loss = 5579463.61758415\n",
      "Iteration 6, loss = 5575364.52611209\n",
      "Iteration 7, loss = 5571268.44617118\n",
      "Iteration 8, loss = 5567175.37558042\n",
      "Iteration 9, loss = 5563085.31199501\n",
      "Iteration 10, loss = 5558998.25328386\n",
      "Iteration 11, loss = 5554914.19713148\n",
      "Iteration 12, loss = 5550833.14155053\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3398.35865627\n",
      "Iteration 2, loss = 3776.54964349\n",
      "Iteration 3, loss = 3775.99313797\n",
      "Iteration 4, loss = 3775.43662922\n",
      "Iteration 5, loss = 3774.88005573\n",
      "Iteration 6, loss = 3774.32360371\n",
      "Iteration 7, loss = 3773.76741913\n",
      "Iteration 8, loss = 3773.21136539\n",
      "Iteration 9, loss = 3772.65510835\n",
      "Iteration 10, loss = 3772.09896544\n",
      "Iteration 11, loss = 3771.54298680\n",
      "Iteration 12, loss = 3770.98705200\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3104.86870656\n",
      "Iteration 2, loss = 3450.24856450\n",
      "Iteration 3, loss = 3449.74002595\n",
      "Iteration 4, loss = 3449.23156671\n",
      "Iteration 5, loss = 3448.72317366\n",
      "Iteration 6, loss = 3448.21472234\n",
      "Iteration 7, loss = 3447.70666538\n",
      "Iteration 8, loss = 3447.19857176\n",
      "Iteration 9, loss = 3446.69039159\n",
      "Iteration 10, loss = 3446.18239731\n",
      "Iteration 11, loss = 3445.67437098\n",
      "Iteration 12, loss = 3445.16660115\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3579.00037221\n",
      "Iteration 2, loss = 3977.22151974\n",
      "Iteration 3, loss = 3976.63543285\n",
      "Iteration 4, loss = 3976.04925245\n",
      "Iteration 5, loss = 3975.46324354\n",
      "Iteration 6, loss = 3974.87726517\n",
      "Iteration 7, loss = 3974.29144615\n",
      "Iteration 8, loss = 3973.70577739\n",
      "Iteration 9, loss = 3973.12019047\n",
      "Iteration 10, loss = 3972.53462537\n",
      "Iteration 11, loss = 3971.94900900\n",
      "Iteration 12, loss = 3971.36363014\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3049.77812368\n",
      "Iteration 2, loss = 3389.11326127\n",
      "Iteration 3, loss = 3388.61376512\n",
      "Iteration 4, loss = 3388.11437597\n",
      "Iteration 5, loss = 3387.61498622\n",
      "Iteration 6, loss = 3387.11575831\n",
      "Iteration 7, loss = 3386.61651297\n",
      "Iteration 8, loss = 3386.11732948\n",
      "Iteration 9, loss = 3385.61849331\n",
      "Iteration 10, loss = 3385.11969444\n",
      "Iteration 11, loss = 3384.62068384\n",
      "Iteration 12, loss = 3384.12162463\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2029.47364155\n",
      "Iteration 2, loss = 2255.17695161\n",
      "Iteration 3, loss = 2254.84461101\n",
      "Iteration 4, loss = 2254.51217628\n",
      "Iteration 5, loss = 2254.18026221\n",
      "Iteration 6, loss = 2253.84797954\n",
      "Iteration 7, loss = 2253.51569862\n",
      "Iteration 8, loss = 2253.18382973\n",
      "Iteration 9, loss = 2252.85195301\n",
      "Iteration 10, loss = 2252.51988792\n",
      "Iteration 11, loss = 2252.18805687\n",
      "Iteration 12, loss = 2251.85607867\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4645.44956941\n",
      "Iteration 2, loss = 4737.17679671\n",
      "Iteration 3, loss = 4733.69705620\n",
      "Iteration 4, loss = 4730.21975312\n",
      "Iteration 5, loss = 4726.74513077\n",
      "Iteration 6, loss = 4723.27295322\n",
      "Iteration 7, loss = 4719.80336661\n",
      "Iteration 8, loss = 4716.33627734\n",
      "Iteration 9, loss = 4712.87184981\n",
      "Iteration 10, loss = 4709.40988294\n",
      "Iteration 11, loss = 4705.95043448\n",
      "Iteration 12, loss = 4702.49355090\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3046.03177543\n",
      "Iteration 2, loss = 3384.75632217\n",
      "Iteration 3, loss = 3384.25746860\n",
      "Iteration 4, loss = 3383.75878857\n",
      "Iteration 5, loss = 3383.26002680\n",
      "Iteration 6, loss = 3382.76139838\n",
      "Iteration 7, loss = 3382.26275363\n",
      "Iteration 8, loss = 3381.76419250\n",
      "Iteration 9, loss = 3381.26574617\n",
      "Iteration 10, loss = 3380.76762840\n",
      "Iteration 11, loss = 3380.26935060\n",
      "Iteration 12, loss = 3379.77095374\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2382264.91489336\n",
      "Iteration 2, loss = 4471994.19421032\n",
      "Iteration 3, loss = 4471368.67815426\n",
      "Iteration 4, loss = 4470709.51670930\n",
      "Iteration 5, loss = 4470050.45218552\n",
      "Iteration 6, loss = 4469391.48500611\n",
      "Iteration 7, loss = 4468732.61464089\n",
      "Iteration 8, loss = 4468073.84155905\n",
      "Iteration 9, loss = 4467415.16575259\n",
      "Iteration 10, loss = 4466756.58674350\n",
      "Iteration 11, loss = 4466098.10531560\n",
      "Iteration 12, loss = 4465439.72042267\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3837.14376312\n",
      "Iteration 2, loss = 4264.16953499\n",
      "Iteration 3, loss = 4263.54121994\n",
      "Iteration 4, loss = 4262.91296219\n",
      "Iteration 5, loss = 4262.28466836\n",
      "Iteration 6, loss = 4261.65634988\n",
      "Iteration 7, loss = 4261.02822126\n",
      "Iteration 8, loss = 4260.40022387\n",
      "Iteration 9, loss = 4259.77229279\n",
      "Iteration 10, loss = 4259.14443356\n",
      "Iteration 11, loss = 4258.51663261\n",
      "Iteration 12, loss = 4257.88906636\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3155.10585359\n",
      "Iteration 2, loss = 3506.16120975\n",
      "Iteration 3, loss = 3505.64460167\n",
      "Iteration 4, loss = 3505.12791143\n",
      "Iteration 5, loss = 3504.61119069\n",
      "Iteration 6, loss = 3504.09470014\n",
      "Iteration 7, loss = 3503.57837633\n",
      "Iteration 8, loss = 3503.06185152\n",
      "Iteration 9, loss = 3502.54551961\n",
      "Iteration 10, loss = 3502.02932691\n",
      "Iteration 11, loss = 3501.51313008\n",
      "Iteration 12, loss = 3500.99723024\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10025.38640682\n",
      "Iteration 2, loss = 11141.24381452\n",
      "Iteration 3, loss = 11139.60180385\n",
      "Iteration 4, loss = 11137.95992254\n",
      "Iteration 5, loss = 11136.31835210\n",
      "Iteration 6, loss = 11134.67695473\n",
      "Iteration 7, loss = 11133.03585747\n",
      "Iteration 8, loss = 11131.39494998\n",
      "Iteration 9, loss = 11129.75413608\n",
      "Iteration 10, loss = 11128.11392148\n",
      "Iteration 11, loss = 11126.47359535\n",
      "Iteration 12, loss = 11124.83377329\n",
      "Training loss did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "capa1 = np.linspace(1, 21, 5)\n",
    "capa2 = np.linspace(0, 20, 5)\n",
    "listarn=[]\n",
    "for c1 in capa1:\n",
    "    for c2 in capa2:\n",
    "        c1=int(c1)\n",
    "        c2=int(c2)\n",
    "        if c2==0:\n",
    "            mlp = MLPClassifier(hidden_layer_sizes=(c1,), max_iter=150, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-3, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "        else:\n",
    "            mlp = MLPClassifier(hidden_layer_sizes=(c1,c2), max_iter=150, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-3, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "        mlp.fit(X_projtrain, y_train)\n",
    "        scores = cross_val_score(mlp, X_projtest, y_test, cv=5)\n",
    "        listarn.append((np.mean(scores),c1,c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6598047793723172, 1, 0),\n",
       " (0.6598047793723172, 1, 5),\n",
       " (0.6598047793723172, 1, 10),\n",
       " (0.6598047793723172, 1, 15),\n",
       " (0.6598047793723172, 1, 20),\n",
       " (0.6598047793723172, 6, 0),\n",
       " (0.6598047793723172, 6, 5),\n",
       " (0.6598047793723172, 6, 10),\n",
       " (0.6598047793723172, 6, 15),\n",
       " (0.6598047793723172, 6, 20),\n",
       " (0.6598047793723172, 11, 0),\n",
       " (0.6598047793723172, 11, 5),\n",
       " (0.6598047793723172, 11, 10),\n",
       " (0.6598047793723172, 11, 15),\n",
       " (0.6598047793723172, 11, 20),\n",
       " (0.6598047793723172, 16, 0),\n",
       " (0.6598047793723172, 16, 5),\n",
       " (0.6598047793723172, 16, 10),\n",
       " (0.659859304999362, 16, 15),\n",
       " (0.6598047793723172, 16, 20),\n",
       " (0.6598047793723172, 21, 0),\n",
       " (0.6598047793723172, 21, 5),\n",
       " (0.6598047793723172, 21, 10),\n",
       " (0.6598047793723172, 21, 15),\n",
       " (0.6597502388759986, 21, 20)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el valor que minimiza el error corresponde a una red neuronal con 16 neuronas en la primera capa y 15 en la segunda. No obstante dado que el valor no cambia mucho al aumentar el número de capas y estas aumentan complejidad al modelo, se considerará como la mejor alternativa de este modelo el correspondiente 1 capa oculta con 1 red neuronal, con un nivel de éxito de 65.98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente el último modelo utilizado corresponde a un algoritmo de boosting (Adaboost), basado en un clasificador debil (Árboles de decisión),el numero de modelos iterativos del clasificador debil entrenado se utilizó como parametro en un gridsearch entre 15 y 50, con el fin de no utilizar algoritmos que tardaran muco en su ejecución. El valor de particiones k para la validación cruzada se mantuvo en 5 para este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6598964  0.66044166 0.66107703 0.66244035 0.65734933]\n",
      "[0.657988   0.66330425 0.66039536 0.66066803 0.65925825]\n",
      "[0.66125954 0.66316794 0.66339468 0.66421268 0.6588492 ]\n",
      "[0.66316794 0.66207743 0.66175869 0.66325835 0.6593946 ]\n",
      "[0.66248637 0.66044166 0.66230402 0.66394001 0.66143987]\n",
      "[0.66166848 0.66139586 0.66257669 0.66298569 0.66116717]\n",
      "[0.6608506  0.65880589 0.66380368 0.66571234 0.66184892]\n",
      "[0.66289531 0.66112323 0.66271302 0.66530334 0.66157622]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "itera=np.linspace(15,50,8)\n",
    "listab=[]\n",
    "for i in itera:\n",
    "    i=int(i)\n",
    "    abc = AdaBoostClassifier(n_estimators=i,learning_rate=1)\n",
    "    model = abc.fit(X_projtrain, y_train)\n",
    "    scoresBoost = cross_val_score(model, X_projtest, y_test, cv=5)\n",
    "    print(scoresBoost)\n",
    "    listab.append((np.mean(scoresBoost),i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.660240954635937, 15),\n",
       " (0.6603227802588109, 20),\n",
       " (0.66217680768138, 25),\n",
       " (0.6619314014788048, 30),\n",
       " (0.6621223861443198, 35),\n",
       " (0.6619587758098536, 40),\n",
       " (0.6622042860922774, 45),\n",
       " (0.6627222237950001, 50)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El mejor algoritmo encontrado corresponde a 50 parametros del algoritmo Adaboost con arboles de decisión con un porcentaje de éxito del 66.27%. Podría encontrarse probablemente mayor éxito al aumentar los parametros, pero esto complciaria el modelo lo cual tampoco es deseable en el alcance de este proyecto.\n",
    "\n",
    "De esta manera se encontró un algoritmo que permite predecir la probabilidad de retweet con mayor éxito (66.27%) corresponde a Adaboost con arboles de decisión como clasificador debil, y un valor de como número de parametros de 50, el cual puede ser usado con el modelo encontrado y la función predict de sklearn (model.predict))\n",
    "\n",
    "De otro lado se encuentra que los tres algoritmos analizados presentaron un porcentaje de exito muy similar y relativamente bajo, esto se debe principalmente a que se redujo la dimensionalidad de las variables en gran medida y se limitaron las iteraciones por algoritmo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
